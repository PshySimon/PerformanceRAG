#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºembeddingçš„è¯­ä¹‰åˆ†å‰²ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨è¯­ä¹‰åˆ†å‰²å™¨å¯¹æ–‡æ¡£è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†ã€‚
è¯­ä¹‰åˆ†å‰²å™¨ä¼šæ ¹æ®æ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ¥å†³å®šåˆ†å‰²ç‚¹ï¼Œ
è€Œä¸æ˜¯ç®€å•åœ°æŒ‰ç…§å­—ç¬¦æ•°æˆ–å¥å­æ•°è¿›è¡Œåˆ†å‰²ã€‚
"""

import sys
from datetime import datetime
from pathlib import Path

from rag.pipeline.factory import create_pipeline
from utils.logger import get_logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

logger = get_logger(__name__)


def save_split_results(documents, output_dir="test_cases/tmp_data"):
    """
    ä¿å­˜åˆ†å‰²ç»“æœåˆ°æŒ‡å®šç›®å½•

    Args:
        documents: åˆ†å‰²åçš„æ–‡æ¡£åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜æ¯ä¸ªåˆ†å‰²å—ä¸ºå•ç‹¬çš„æ–‡ä»¶
    saved_files = []
    for i, doc in enumerate(documents):
        # æ–‡ä»¶åæ ¼å¼ï¼šchunk_åºå·_æ—¶é—´æˆ³.txt
        filename = f"chunk_{i+1:03d}_{timestamp}.txt"
        file_path = output_path / filename

        # å†™å…¥æ–‡ä»¶å†…å®¹ï¼ˆåªä¿å­˜çº¯æ–‡æœ¬ï¼‰
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(doc["content"])

        saved_files.append(filename)

    logger.info(f"åˆ†å‰²ç»“æœå·²ä¿å­˜åˆ° {output_path}")
    logger.info(f"å…±ä¿å­˜ {len(documents)} ä¸ªtxtæ–‡ä»¶")

    return output_path, saved_files


def run_semantic_splitting_example():
    """
    è¿è¡Œè¯­ä¹‰åˆ†å‰²ç¤ºä¾‹
    """
    logger.info("å¼€å§‹è¿è¡ŒåŸºäºembeddingçš„è¯­ä¹‰åˆ†å‰²ç¤ºä¾‹")

    try:
        # 2. åˆ›å»ºå¹¶é…ç½®Pipeline
        logger.info("åˆ›å»ºè¯­ä¹‰åˆ†å‰²Pipeline...")
        pipeline = create_pipeline("semantic_splitter_example")

        # 3. æ‰§è¡Œæ–‡æ¡£åŠ è½½å’Œåˆ†å‰²
        logger.info("å¼€å§‹æ‰§è¡Œæ–‡æ¡£åˆ†å‰²...")
        result = pipeline.run(input_data={}, entry_point="document_loader")

        # 4. åˆ†æåˆ†å‰²ç»“æœ
        if "documents" in result:
            documents = result["documents"]
            logger.info(f"åˆ†å‰²å®Œæˆï¼å…±ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—")

            # æ˜¾ç¤ºåˆ†å‰²ç»“æœç»Ÿè®¡
            print("\n=== è¯­ä¹‰åˆ†å‰²ç»“æœç»Ÿè®¡ ===")
            print("åŸå§‹æ–‡æ¡£æ•°é‡: 1")
            print(f"åˆ†å‰²åå—æ•°é‡: {len(documents)}")

            # åˆ†ææ¯ä¸ªå—çš„å¤§å°
            chunk_sizes = [len(doc["content"]) for doc in documents]
            print(f"å¹³å‡å—å¤§å°: {sum(chunk_sizes) / len(chunk_sizes):.1f} å­—ç¬¦")
            print(f"æœ€å°å—å¤§å°: {min(chunk_sizes)} å­—ç¬¦")
            print(f"æœ€å¤§å—å¤§å°: {max(chunk_sizes)} å­—ç¬¦")

            # æ˜¾ç¤ºå‰å‡ ä¸ªåˆ†å‰²å—çš„å†…å®¹é¢„è§ˆ
            print("\n=== åˆ†å‰²å—å†…å®¹é¢„è§ˆ ===")
            for i, doc in enumerate(documents[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªå—
                content_preview = (
                    doc["content"][:100] + "..."
                    if len(doc["content"]) > 100
                    else doc["content"]
                )
                print(f"\nå— {i+1} (é•¿åº¦: {len(doc['content'])} å­—ç¬¦):")
                print(f"å†…å®¹: {content_preview}")
                if "metadata" in doc:
                    print(f"å…ƒæ•°æ®: {doc['metadata']}")

            if len(documents) > 3:
                print(f"\n... è¿˜æœ‰ {len(documents) - 3} ä¸ªå—æœªæ˜¾ç¤º")

            # ä¿å­˜åˆ†å‰²ç»“æœåˆ°æ–‡ä»¶
            print("\n=== ä¿å­˜åˆ†å‰²ç»“æœ ===")
            output_path, saved_files = save_split_results(documents)
            print(f"âœ… åˆ†å‰²ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            print(
                f"ğŸ“„ ä¿å­˜çš„æ–‡ä»¶: {', '.join(saved_files[:3])}{'...' if len(saved_files) > 3 else ''}"
            )

            return documents, output_path

        else:
            logger.error("åˆ†å‰²ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°documentså­—æ®µ")
            print(f"ç»“æœå†…å®¹: {result}")
            return None, None

    except Exception as e:
        logger.error(f"è¯­ä¹‰åˆ†å‰²ç¤ºä¾‹æ‰§è¡Œå¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        return None, None


def compare_splitting_methods():
    """
    æ¯”è¾ƒä¸åŒåˆ†å‰²æ–¹æ³•çš„æ•ˆæœ
    """
    logger.info("æ¯”è¾ƒä¸åŒåˆ†å‰²æ–¹æ³•çš„æ•ˆæœ")

    # è¿™é‡Œå¯ä»¥æ‰©å±•ï¼Œæ¯”è¾ƒè¯­ä¹‰åˆ†å‰²ä¸å…¶ä»–åˆ†å‰²æ–¹æ³•çš„å·®å¼‚
    # ä¾‹å¦‚ï¼šæ–‡æœ¬åˆ†å‰²ã€é€’å½’åˆ†å‰²ç­‰
    pass


def main():
    """
    ä¸»å‡½æ•°
    """
    print("åŸºäºEmbeddingçš„è¯­ä¹‰åˆ†å‰²ç¤ºä¾‹")
    print("=" * 50)

    # æ£€æŸ¥é…ç½®
    print("\nè¯·ç¡®ä¿å·²æ­£ç¡®é…ç½®ä»¥ä¸‹å†…å®¹:")
    print("1. åœ¨ config/semantic_splitter_example.yaml ä¸­è®¾ç½®æ­£ç¡®çš„APIå¯†é’¥")
    print("2. ç¡®ä¿embeddingæœåŠ¡å¯ç”¨ï¼ˆOpenAI APIæˆ–æœ¬åœ°æœåŠ¡ï¼‰")
    print("3. å°†ä½ çš„æ–‡æ¡£æ”¾åœ¨ ./test_cases/test_data/ ç›®å½•ä¸‹")

    input("\næŒ‰å›è½¦é”®ç»§ç»­...")

    # è¿è¡Œç¤ºä¾‹
    documents, output_path = run_semantic_splitting_example()

    if documents and output_path:
        print("\nâœ… è¯­ä¹‰åˆ†å‰²ç¤ºä¾‹æ‰§è¡ŒæˆåŠŸï¼")
        print(f"ğŸ“ åˆ†å‰²æ–‡ä»¶ä¿å­˜åœ¨: {output_path}")
        print(f"ğŸ“„ å…±ç”Ÿæˆ {len(documents)} ä¸ªtxtæ–‡ä»¶")
        print("\nğŸ’¡ æç¤º:")
        print("- è¯­ä¹‰åˆ†å‰²ä¼šæ ¹æ®æ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ™ºèƒ½åˆ†å‰²")
        print("- ç›¸ä¼¼ä¸»é¢˜çš„å¥å­ä¼šè¢«ä¿æŒåœ¨åŒä¸€ä¸ªå—ä¸­")
        print("- å¯ä»¥é€šè¿‡è°ƒæ•´ similarity_threshold æ¥æ§åˆ¶åˆ†å‰²ç²’åº¦")
        print("- è¾ƒé«˜çš„é˜ˆå€¼ä¼šäº§ç”Ÿæ›´å¤šã€æ›´å°çš„å—")
        print("- è¾ƒä½çš„é˜ˆå€¼ä¼šäº§ç”Ÿæ›´å°‘ã€æ›´å¤§çš„å—")
        print("- åˆ†å‰²ç»“æœå·²ä¿å­˜åˆ° test_cases/tmp_data ç›®å½•")
    else:
        print("\nâŒ è¯­ä¹‰åˆ†å‰²ç¤ºä¾‹æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œæ—¥å¿—")


if __name__ == "__main__":
    main()
