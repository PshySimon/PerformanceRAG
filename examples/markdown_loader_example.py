#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Markdown æ–‡æ¡£åŠ è½½ç¤ºä¾‹ - é‡ç‚¹å±•ç¤º Metadata
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¼˜åŒ–åçš„ FileLoader æŒ‰æ ‡é¢˜+æ®µè½åˆ‡åˆ† Markdown æ–‡æ¡£
"""

import json
import os
import sys
from pathlib import Path
from typing import Any, Dict, List

from llama_index.core import Document
from llama_index.core.node_parser import MarkdownNodeParser

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class EnhancedFileLoader:
    """å¢å¼ºç‰ˆæ–‡ä»¶åŠ è½½å™¨ï¼Œæ”¯æŒ Markdown ç»“æ„åŒ–è§£æ"""

    def __init__(self, enable_markdown_parsing: bool = True):
        """åˆå§‹åŒ–åŠ è½½å™¨"""
        self.enable_markdown_parsing = enable_markdown_parsing

        if self.enable_markdown_parsing:
            self.markdown_parser = MarkdownNodeParser()

    def _extract_heading_context(self, nodes: List[Any]) -> List[Dict[str, Any]]:
        """æå–èŠ‚ç‚¹çš„æ ‡é¢˜ä¸Šä¸‹æ–‡ï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ å¯¹åº”çš„æœ€å°çº§åˆ«æ ‡é¢˜"""
        documents = []
        current_headings = {}  # å­˜å‚¨å½“å‰å„çº§åˆ«çš„æ ‡é¢˜

        for node in nodes:
            metadata = node.metadata.copy()
            text = node.text.strip()

            # å¦‚æœæ˜¯æ ‡é¢˜èŠ‚ç‚¹
            if "heading_level" in metadata:
                heading_level = metadata["heading_level"]
                # æ›´æ–°å½“å‰æ ‡é¢˜å±‚çº§
                current_headings[heading_level] = text
                # æ¸…é™¤æ›´æ·±å±‚çº§çš„æ ‡é¢˜
                keys_to_remove = [
                    k for k in current_headings.keys() if k > heading_level
                ]
                for k in keys_to_remove:
                    del current_headings[k]

                # ä¸ºæ ‡é¢˜èŠ‚ç‚¹æ·»åŠ æ ‡é¢˜è·¯å¾„
                heading_path = []
                for level in sorted(current_headings.keys()):
                    heading_path.append(current_headings[level])
                metadata["heading_path"] = heading_path
                metadata["current_heading"] = text
                metadata["is_heading"] = True
                metadata["node_type"] = "heading"
            else:
                # ä¸ºæ­£æ–‡èŠ‚ç‚¹æ·»åŠ å¯¹åº”çš„æœ€å°çº§åˆ«æ ‡é¢˜
                if current_headings:
                    # è·å–æœ€æ·±å±‚çº§çš„æ ‡é¢˜ä½œä¸ºå½“å‰æ®µè½çš„æ ‡é¢˜
                    max_level = max(current_headings.keys())
                    metadata["current_heading"] = current_headings[max_level]
                    metadata["heading_level"] = max_level

                    # æ·»åŠ å®Œæ•´çš„æ ‡é¢˜è·¯å¾„
                    heading_path = []
                    for level in sorted(current_headings.keys()):
                        heading_path.append(current_headings[level])
                    metadata["heading_path"] = heading_path
                else:
                    metadata["current_heading"] = None
                    metadata["heading_path"] = []

                metadata["is_heading"] = False
                metadata["node_type"] = "content"

            # æ·»åŠ å†…å®¹ç»Ÿè®¡ä¿¡æ¯
            metadata["content_length"] = len(text)
            metadata["word_count"] = len(text.split())
            metadata["line_count"] = len(text.split("\n"))

            documents.append({"content": text, "metadata": metadata})

        return documents

    def load_markdown_file(self, file_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½ Markdown æ–‡ä»¶å¹¶æŒ‰æ ‡é¢˜+æ®µè½åˆ‡åˆ†"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # åˆ›å»º Document å¯¹è±¡
        doc = Document(
            text=content,
            metadata={
                "source": file_path,
                "file_type": ".md",
                "file_name": os.path.basename(file_path),
            },
        )

        # ä½¿ç”¨ MarkdownNodeParser è§£æ
        nodes = self.markdown_parser.get_nodes_from_documents([doc])

        # æå–æ ‡é¢˜ä¸Šä¸‹æ–‡å¹¶è¿”å›
        return self._extract_heading_context(nodes)


def print_metadata_focus(documents: List[Dict[str, Any]]):
    """é‡ç‚¹æ‰“å°æ¯ä¸ªèŠ‚ç‚¹çš„ Metadata ä¿¡æ¯"""
    print(f"\n{'='*100}")
    print(f"ğŸ” METADATA è¯¦ç»†åˆ†æ - æ€»å…± {len(documents)} ä¸ªèŠ‚ç‚¹")
    print(f"{'='*100}\n")

    for i, doc in enumerate(documents, 1):
        content = doc["content"]
        metadata = doc["metadata"]

        print(f"\nğŸ“‹ èŠ‚ç‚¹ {i} - METADATA è¯¦æƒ…")
        print(f"{'='*80}")

        # ğŸ¯ æ ¸å¿ƒ Metadata ä¿¡æ¯
        print("\nğŸ¯ ã€æ ¸å¿ƒä¿¡æ¯ã€‘")
        print(f"   â€¢ node_type: {metadata.get('node_type', 'unknown')}")
        print(f"   â€¢ is_heading: {metadata.get('is_heading', False)}")
        print(f"   â€¢ current_heading: {metadata.get('current_heading', 'None')}")

        if "heading_level" in metadata:
            print(f"   â€¢ heading_level: {metadata['heading_level']}")

        # ğŸ—‚ï¸ æ ‡é¢˜è·¯å¾„ä¿¡æ¯
        heading_path = metadata.get("heading_path", [])
        if heading_path:
            print("\nğŸ—‚ï¸ ã€æ ‡é¢˜è·¯å¾„ã€‘")
            for idx, path_item in enumerate(heading_path):
                indent = "   " + "  " * idx
                print(f"{indent}â””â”€ H{idx+1}: {path_item}")

        # ğŸ“Š å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š ã€å†…å®¹ç»Ÿè®¡ã€‘")
        print(f"   â€¢ content_length: {metadata.get('content_length', 0)} å­—ç¬¦")
        print(f"   â€¢ word_count: {metadata.get('word_count', 0)} è¯")
        print(f"   â€¢ line_count: {metadata.get('line_count', 0)} è¡Œ")

        # ğŸ“ æ–‡ä»¶ä¿¡æ¯
        print("\nğŸ“ ã€æ–‡ä»¶ä¿¡æ¯ã€‘")
        print(f"   â€¢ source: {metadata.get('source', 'Unknown')}")
        print(f"   â€¢ file_type: {metadata.get('file_type', 'Unknown')}")
        print(f"   â€¢ file_name: {metadata.get('file_name', 'Unknown')}")

        # ğŸ”§ åŸå§‹ llama_index metadata
        llama_metadata = {
            k: v
            for k, v in metadata.items()
            if k
            not in [
                "node_type",
                "is_heading",
                "current_heading",
                "heading_level",
                "heading_path",
                "content_length",
                "word_count",
                "line_count",
                "source",
                "file_type",
                "file_name",
            ]
        }
        if llama_metadata:
            print("\nğŸ”§ ã€åŸå§‹ LlamaIndex Metadataã€‘")
            for key, value in llama_metadata.items():
                print(f"   â€¢ {key}: {value}")

        # ğŸ“„ å†…å®¹é¢„è§ˆ
        content_preview = content.replace("\n", " ").strip()
        if len(content_preview) > 80:
            content_preview = content_preview[:80] + "..."
        print("\nğŸ“„ ã€å†…å®¹é¢„è§ˆã€‘")
        print(f"   {content_preview}")

        # ğŸ“‹ å®Œæ•´ Metadata JSON
        print("\nğŸ“‹ ã€å®Œæ•´ Metadata JSONã€‘")
        metadata_json = json.dumps(metadata, ensure_ascii=False, indent=2)
        print(f"```json\n{metadata_json}\n```")

        print(f"\n{'-'*80}\n")


def analyze_metadata_patterns(documents: List[Dict[str, Any]]):
    """åˆ†æ Metadata æ¨¡å¼å’Œç»Ÿè®¡"""
    print(f"\n{'='*100}")
    print("ğŸ“ˆ METADATA æ¨¡å¼åˆ†æ")
    print(f"{'='*100}")

    # ç»Ÿè®¡ä¸åŒç±»å‹çš„èŠ‚ç‚¹
    headings = [doc for doc in documents if doc["metadata"].get("is_heading", False)]
    contents = [
        doc for doc in documents if not doc["metadata"].get("is_heading", False)
    ]

    print("\nğŸ“Š èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:")
    print(f"   â€¢ æ€»èŠ‚ç‚¹æ•°: {len(documents)}")
    print(f"   â€¢ æ ‡é¢˜èŠ‚ç‚¹: {len(headings)}")
    print(f"   â€¢ å†…å®¹èŠ‚ç‚¹: {len(contents)}")

    # ç»Ÿè®¡æ ‡é¢˜å±‚çº§åˆ†å¸ƒ
    heading_levels = {}
    for doc in headings:
        level = doc["metadata"].get("heading_level", 0)
        heading_levels[level] = heading_levels.get(level, 0) + 1

    if heading_levels:
        print("\nğŸ·ï¸ æ ‡é¢˜å±‚çº§åˆ†å¸ƒ:")
        for level in sorted(heading_levels.keys()):
            count = heading_levels[level]
            print(f"   â€¢ H{level}: {count} ä¸ªæ ‡é¢˜")

    # åˆ†æå†…å®¹é•¿åº¦åˆ†å¸ƒ
    content_lengths = [doc["metadata"].get("content_length", 0) for doc in documents]
    if content_lengths:
        print("\nğŸ“ å†…å®¹é•¿åº¦ç»Ÿè®¡:")
        print(f"   â€¢ æœ€çŸ­: {min(content_lengths)} å­—ç¬¦")
        print(f"   â€¢ æœ€é•¿: {max(content_lengths)} å­—ç¬¦")
        print(f"   â€¢ å¹³å‡: {sum(content_lengths) // len(content_lengths)} å­—ç¬¦")

    # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„ metadata å­—æ®µ
    all_metadata_keys = set()
    for doc in documents:
        all_metadata_keys.update(doc["metadata"].keys())

    print("\nğŸ”‘ æ‰€æœ‰ Metadata å­—æ®µ:")
    for key in sorted(all_metadata_keys):
        print(f"   â€¢ {key}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Markdown Metadata è¯¦ç»†åˆ†æå™¨")
    print("é‡ç‚¹å±•ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„ Metadata ä¿¡æ¯\n")

    # æŒ‡å®šè¦åŠ è½½çš„æ–‡ä»¶
    file_path = "/Users/caixiaomeng/Projects/Python/PerformanceRag/test_cases/test_data/recursive_splitter_data.md"

    try:
        # åˆ›å»ºåŠ è½½å™¨
        loader = EnhancedFileLoader(enable_markdown_parsing=True)

        # åŠ è½½æ–‡æ¡£
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ–‡ä»¶: {file_path}")
        documents = loader.load_markdown_file(file_path)

        # é‡ç‚¹æ‰“å° Metadata ä¿¡æ¯
        print_metadata_focus(documents)

        # åˆ†æ Metadata æ¨¡å¼
        analyze_metadata_patterns(documents)

        print("\nâœ… Metadata åˆ†æå®Œæˆï¼")
        print(
            "ğŸ’¡ å…³é”®ä¿¡æ¯: æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«äº†ä¸°å¯Œçš„ metadataï¼ŒåŒ…æ‹¬æ ‡é¢˜è·¯å¾„ã€å±‚çº§ã€å†…å®¹ç»Ÿè®¡ç­‰"
        )

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
