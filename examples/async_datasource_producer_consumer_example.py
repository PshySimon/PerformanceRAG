#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚æ­¥æ•°æ®æº-ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ‰§è¡Œå™¨ä½¿ç”¨ç¤ºä¾‹
æ”¯æŒSmall2Bigæ£€ç´¢ç­–ç•¥

æ•°æ®æº: loader + splitter (æ”¯æŒå±‚çº§åˆ†å‰²)
ç”Ÿäº§è€…: embedding
æ¶ˆè´¹è€…: indexer (æ”¯æŒå±‚çº§ç´¢å¼•)
"""

import asyncio
import logging
import os
import sys
import time

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag.pipeline.async_datasource_producer_consumer_executor import (
    AsyncDataSourceProducerConsumerPipelineExecutor,
)
from rag.components.retriever.es_retriever import ESRetrieverComponent
from utils.config import config
from utils.logger import get_logger, setup_logging

setup_logging(level="INFO")
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("openai._base_client").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("elastic_transport").setLevel(logging.WARNING)
logging.getLogger("elastic_transport.transport").setLevel(logging.WARNING)
logging.getLogger("elasticsearch").setLevel(logging.WARNING)


async def async_indexing_example():
    """å¼‚æ­¥ç´¢å¼•ç¤ºä¾‹ - æ”¯æŒå±‚çº§åˆ†å‰²å’Œç´¢å¼•"""
    logger = get_logger(__name__)
    logger.info("=== å¼‚æ­¥æ•°æ®æº-ç”Ÿäº§è€…-æ¶ˆè´¹è€…Pipelineç¤ºä¾‹ (æ”¯æŒSmall2Big) ===\n")

    # ç›´æ¥ä½¿ç”¨ utils.configï¼Œæ— éœ€ dict() åŒ…è£…
    pipeline_config = config.datasource_producer_consumer_pipeline

    # åˆ›å»ºå¼‚æ­¥æ‰§è¡Œå™¨
    executor = AsyncDataSourceProducerConsumerPipelineExecutor(pipeline_config)

    # å¼‚æ­¥æ‰§è¡Œç´¢å¼•
    start_time = time.time()
    result = await executor.run_async({})
    end_time = time.time()

    logger.info("\nğŸ¯ å¼‚æ­¥ç´¢å¼•æ‰§è¡Œå®Œæˆï¼")
    logger.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
    logger.info(f"å¤„ç†ç»“æœ: {result}")
    
    return result


async def async_search_example():
    """å¼‚æ­¥æ£€ç´¢ç¤ºä¾‹ - æ¼”ç¤ºSmall2Bigæ£€ç´¢ç­–ç•¥"""
    logger = get_logger(__name__)
    logger.info("\n=== Small2Bigæ£€ç´¢ç­–ç•¥æ¼”ç¤º ===\n")
    
    # åŠ è½½æ£€ç´¢é…ç½®
    # ä¿®æ”¹ç¬¬69-70è¡Œ
    search_config = config.es_search_pipeline
    retriever_config = search_config['components']['es_retriever']['config']
    
    # åˆ›å»ºæ£€ç´¢å™¨ - æ·»åŠ ç¼ºå°‘çš„nameå‚æ•°
    retriever = ESRetrieverComponent("es_retriever", retriever_config)
    retriever.initialize()  # æ˜¾å¼åˆå§‹åŒ–
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Pythonå¼‚æ­¥ç¼–ç¨‹çš„æœ€ä½³å®è·µ",
        "å¦‚ä½•ä¼˜åŒ–Elasticsearchæ€§èƒ½",
        "RAGç³»ç»Ÿçš„æ¶æ„è®¾è®¡"
    ]
    
    for i, query in enumerate(test_queries, 1):
        logger.info(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ #{i}: {query}")
        
        start_time = time.time()
        results = retriever.retrieve(query, top_k=5)
        end_time = time.time()
        
        logger.info(f"â±ï¸ æ£€ç´¢è€—æ—¶: {end_time - start_time:.3f}ç§’")
        logger.info(f"ğŸ“Š è¿”å›ç»“æœæ•°é‡: {len(results)}")
        
        # æ˜¾ç¤ºæ£€ç´¢ç»“æœè¯¦æƒ…
        for j, result in enumerate(results, 1):
            content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
            
            logger.info(f"\nğŸ“„ ç»“æœ #{j}:")
            logger.info(f"   ğŸ“‹ æ–‡æ¡£ID: {result['id']}")
            logger.info(f"   â­ åˆ†æ•°: {result['score']:.4f}")
            logger.info(f"   ğŸ·ï¸ å¬å›æ–¹å¼: {result['recall_source']}")
            
            if result['recall_source'] == 'small2big':
                logger.info(f"   ğŸ”— æ¥æºSmall Chunkæ•°: {result.get('source_small_chunks_count', 0)}")
                logger.info(f"   ğŸ“ˆ æœ€é«˜Smallåˆ†æ•°: {result.get('max_small_score', 0):.4f}")
                logger.info(f"   ğŸ“Š å¹³å‡Smallåˆ†æ•°: {result.get('avg_small_score', 0):.4f}")
                logger.info(f"   ğŸ”„ èåˆæ–¹æ³•: {result.get('fusion_method', 'unknown')}")
            
            logger.info(f"   ğŸ“ å†…å®¹é¢„è§ˆ: {content_preview}")
            
            # æ˜¾ç¤ºå…ƒæ•°æ®
            metadata = result.get('metadata', {})
            if metadata:
                logger.info(f"   ğŸ·ï¸ æ–‡ä»¶è·¯å¾„: {metadata.get('file_path', 'N/A')}")
                logger.info(f"   ğŸ“ Chunkå±‚çº§: {metadata.get('chunk_level', 'N/A')}")
                logger.info(f"   ğŸ“¦ Chunkå¤§å°: {metadata.get('chunk_size', 'N/A')}")


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´çš„Small2Bigæµç¨‹"""
    logger = get_logger(__name__)
    
    try:
        # è¯¢é—®ç”¨æˆ·è¦æ‰§è¡Œçš„æ“ä½œ
        print("\nè¯·é€‰æ‹©è¦æ‰§è¡Œçš„æ“ä½œ:")
        print("1. æ‰§è¡Œç´¢å¼• (æ•°æ®æº->ç”Ÿäº§è€…->æ¶ˆè´¹è€…)")
        print("2. æ‰§è¡Œæ£€ç´¢ (Small2Bigç­–ç•¥æ¼”ç¤º)")
        print("3. æ‰§è¡Œå®Œæ•´æµç¨‹ (ç´¢å¼•+æ£€ç´¢)")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
        
        if choice == "1":
            await async_indexing_example()
        elif choice == "2":
            await async_search_example()
        elif choice == "3":
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æµç¨‹...\n")
            
            # å…ˆæ‰§è¡Œç´¢å¼•
            await async_indexing_example()
            
            # ç­‰å¾…ç´¢å¼•å®Œæˆ
            logger.info("\nâ³ ç­‰å¾…ç´¢å¼•åˆ·æ–°...")
            await asyncio.sleep(5)
            
            # å†æ‰§è¡Œæ£€ç´¢
            await async_search_example()
            
            logger.info("\nğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆï¼")
        else:
            logger.warning("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé€€å‡ºç¨‹åº")
            
    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    print("ğŸš€ å¯åŠ¨Small2Bigæ£€ç´¢ç­–ç•¥æ¼”ç¤ºç¨‹åº...")
    asyncio.run(main())

