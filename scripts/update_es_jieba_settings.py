#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åœ¨çº¿æ›´æ–°ESç´¢å¼•è®¾ç½®ä»¥æ”¯æŒjiebaåˆ†è¯
æ— éœ€é‡æ–°ç´¢å¼•æ•°æ®çš„è§£å†³æ–¹æ¡ˆ
"""

import os
import sys
from typing import Any, Dict

from elasticsearch import Elasticsearch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.logger import get_logger, setup_logging

setup_logging(level="INFO")
logger = get_logger(__name__)

# ESè¿æ¥é…ç½®
ES_CONFIG = {
    "hosts": ["https://localhost:9200"],
    "basic_auth": ("elastic", "sPxLec=NGSFmUT_7+74R"),
    "verify_certs": False,
    "timeout": 60,
}

ORIGINAL_INDEX = "vector_performance_docs"
NEW_INDEX_NAME = "vector_performance_docs_jieba"
ALIAS_NAME = "vector_performance_docs_current"  # ä½¿ç”¨ä¸åŒçš„åˆ«åå


def create_jieba_settings() -> Dict[str, Any]:
    """åˆ›å»ºæ”¯æŒIKåˆ†è¯çš„ç´¢å¼•è®¾ç½®"""
    return {
        "analysis": {
            "analyzer": {
                "ik_analyzer": {
                    "type": "ik_max_word"
                },
                "ik_search_analyzer": {
                    "type": "ik_smart"
                }
            }
        },
        "number_of_shards": 1,
        "number_of_replicas": 0
    }

def create_jieba_mapping() -> Dict[str, Any]:
    """åˆ›å»ºæ”¯æŒIKåˆ†è¯çš„å­—æ®µæ˜ å°„"""
    return {
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_analyzer",
                "search_analyzer": "ik_search_analyzer"
            },
            "content_jieba": {
                "type": "text",
                "analyzer": "ik_analyzer",
                "search_analyzer": "ik_search_analyzer"
            },
            "metadata": {
                "type": "object",
                "properties": {
                    "source": {
                        "type": "text",
                        "analyzer": "ik_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "file_path": {
                        "type": "text",
                        "analyzer": "ik_analyzer"
                    },
                    "filename": {
                        "type": "keyword",
                        "index": False      # filenameä¸ç´¢å¼•
                    },
                    "file_type": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "total_chunks": {"type": "integer"},
                    "chunk_size": {"type": "integer"},
                    "split_method": {"type": "keyword"}
                }
            },
            "content_vector": {
                "type": "dense_vector",
                "dims": 1024,
                "index": True,
                "similarity": "cosine"
            },
            "timestamp": {"type": "date"}
        }
    }


def preprocess_with_jieba(text: str) -> str:
    """ä½¿ç”¨jiebaé¢„å¤„ç†æ–‡æœ¬"""
    try:
        import jieba
        import jieba.analyse

        jieba.setLogLevel(20)  # å‡å°‘æ—¥å¿—è¾“å‡º

        # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
        tokens = list(jieba.cut(text.strip()))
        # è¿‡æ»¤ç©ºç™½ã€å•å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
        filtered_tokens = []
        for token in tokens:
            token = token.strip()
            if token and len(token) > 1 and not token.isspace():
                # è¿‡æ»¤çº¯æ ‡ç‚¹ç¬¦å·
                if not all(ord(char) < 128 and not char.isalnum() for char in token):
                    filtered_tokens.append(token)

        return " ".join(filtered_tokens)
    except ImportError:
        logger.error("è¯·å®‰è£…jieba: pip install jieba")
        return text
    except Exception as e:
        logger.error(f"jiebaåˆ†è¯å¤±è´¥: {e}")
        return text


# åˆ é™¤è¿™ä¸ªå‡½æ•°ï¼Œä¸éœ€è¦äº†
# def preprocess_with_jieba(text: str) -> str:


# åœ¨update_index_with_jieba()å‡½æ•°ä¸­ï¼Œç®€åŒ–ä¸ºç›´æ¥reindex
def update_index_with_jieba():
    try:
        es = Elasticsearch(**ES_CONFIG)
        logger.info("è¿æ¥åˆ°ElasticsearchæˆåŠŸ")

        # æ£€æŸ¥åŸç´¢å¼•æ˜¯å¦å­˜åœ¨
        if not es.indices.exists(index=ORIGINAL_INDEX):
            logger.error(f"åŸç´¢å¼• {ORIGINAL_INDEX} ä¸å­˜åœ¨")
            return False

        # è·å–åŸç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        stats = es.indices.stats(index=ORIGINAL_INDEX)
        total_docs = stats["indices"][ORIGINAL_INDEX]["total"]["docs"]["count"]
        logger.info(f"åŸç´¢å¼•æ–‡æ¡£æ•°: {total_docs:,}")

        # 1. åˆ›å»ºæ–°ç´¢å¼•ï¼ˆæ”¯æŒjiebaï¼‰
        logger.info(f"åˆ›å»ºæ–°ç´¢å¼• {NEW_INDEX_NAME}...")
        if es.indices.exists(index=NEW_INDEX_NAME):
            logger.info(f"åˆ é™¤å·²å­˜åœ¨çš„ç´¢å¼• {NEW_INDEX_NAME}")
            es.indices.delete(index=NEW_INDEX_NAME)

        es.indices.create(
            index=NEW_INDEX_NAME,
            body={
                "settings": create_jieba_settings(),
                "mappings": create_jieba_mapping(),
            },
        )
        logger.info(f"æ–°ç´¢å¼• {NEW_INDEX_NAME} åˆ›å»ºæˆåŠŸ")

        # 2. ç›´æ¥ä½¿ç”¨Reindex APIå¤åˆ¶æ•°æ®ï¼Œè®©ES jiebaæ’ä»¶å¤„ç†åˆ†è¯
        logger.info("å¼€å§‹å¤åˆ¶æ•°æ®...")
        reindex_body = {
            "source": {
                "index": ORIGINAL_INDEX
            },
            "dest": {
                "index": NEW_INDEX_NAME
            }
        }
        
        reindex_response = es.reindex(body=reindex_body, wait_for_completion=True)
        logger.info(f"æ•°æ®å¤åˆ¶å®Œæˆï¼Œå¤„ç†äº† {reindex_response.get('total', 0)} ä¸ªæ–‡æ¡£")
        
        # åˆ é™¤æ‰€æœ‰jiebaé¢„å¤„ç†çš„ä»£ç ï¼Œç›´æ¥è·³åˆ°åˆ«ååˆ›å»º
        # 3. ä½¿ç”¨Update by Queryä¸ºcontentå­—æ®µæ·»åŠ jiebaåˆ†è¯ç»“æœ
        logger.info("å¼€å§‹ä¸ºæ–‡æ¡£æ·»åŠ jiebaåˆ†è¯ç»“æœ...")

        # åˆ†æ‰¹å¤„ç†æ–‡æ¡£
        batch_size = 100
        processed = 0

        # æ»šåŠ¨æŸ¥è¯¢æ‰€æœ‰æ–‡æ¡£
        scroll_response = es.search(
            index=NEW_INDEX_NAME,
            scroll="2m",
            size=batch_size,
            body={"query": {"match_all": {}}},
        )

        scroll_id = scroll_response["_scroll_id"]
        hits = scroll_response["hits"]["hits"]

        while hits:
            # å‡†å¤‡æ‰¹é‡æ›´æ–°æ“ä½œ
            bulk_body = []

            for hit in hits:
                doc_id = hit["_id"]
                content = hit["_source"].get("content", "")
                metadata = hit["_source"].get("metadata", {})

                # ä½¿ç”¨jiebaå¤„ç†content
                content_jieba = preprocess_with_jieba(content)

                # åªå¤„ç†file_pathï¼Œä¸å¤„ç†filename
                if "file_path" in metadata:
                    metadata["file_path_jieba"] = preprocess_with_jieba(
                        str(metadata["file_path"])
                    )

                # æ·»åŠ æ›´æ–°æ“ä½œ
                bulk_body.append({"update": {"_index": NEW_INDEX_NAME, "_id": doc_id}})
                bulk_body.append(
                    {"doc": {"content_jieba": content_jieba, "metadata": metadata}}
                )

            # æ‰§è¡Œæ‰¹é‡æ›´æ–°
            if bulk_body:
                es.bulk(body=bulk_body)
                processed += len(hits)
                logger.info(f"å·²å¤„ç† {processed}/{total_docs} ä¸ªæ–‡æ¡£")

            # è·å–ä¸‹ä¸€æ‰¹
            scroll_response = es.scroll(scroll_id=scroll_id, scroll="2m")
            hits = scroll_response["hits"]["hits"]

        # æ¸…ç†scroll
        es.clear_scroll(scroll_id=scroll_id)

        # 4. åˆ›å»ºåˆ«åæŒ‡å‘æ–°ç´¢å¼•
        logger.info("åˆ›å»ºç´¢å¼•åˆ«å...")

        # åˆ é™¤å¯èƒ½å­˜åœ¨çš„æ—§åˆ«å
        if es.indices.exists_alias(name=ALIAS_NAME):
            aliases = es.indices.get_alias(name=ALIAS_NAME)
            old_indices = list(aliases.keys())
            for old_index in old_indices:
                es.indices.delete_alias(index=old_index, name=ALIAS_NAME)
            logger.info(f"åˆ é™¤æ—§åˆ«å {ALIAS_NAME}")

        # åˆ›å»ºæ–°åˆ«å
        es.indices.put_alias(index=NEW_INDEX_NAME, name=ALIAS_NAME)
        logger.info(f"åˆ›å»ºåˆ«å {ALIAS_NAME} æŒ‡å‘ {NEW_INDEX_NAME}")

        # 5. éªŒè¯ç»“æœ
        logger.info("éªŒè¯jiebaåˆ†è¯æ•ˆæœ...")
        test_query = "æ€§èƒ½åˆ†æå·¥å…·"

        # æµ‹è¯•jiebaåˆ†è¯æŸ¥è¯¢
        response = es.search(
            index=ALIAS_NAME,
            body={
                "query": {"match": {"content_jieba": test_query}},
                "size": 3,
                "highlight": {"fields": {"content_jieba": {}}},
            },
        )

        logger.info(
            f"æµ‹è¯•æŸ¥è¯¢ '{test_query}' è¿”å› {len(response['hits']['hits'])} ä¸ªç»“æœ"
        )
        for hit in response["hits"]["hits"][:2]:
            logger.info(f"æ–‡æ¡£ID: {hit['_id']}, åˆ†æ•°: {hit['_score']:.3f}")
            if "highlight" in hit:
                logger.info(f"é«˜äº®: {hit['highlight']}")

        logger.info("âœ… jiebaåˆ†è¯æ›´æ–°å®Œæˆï¼")
        logger.info("\nğŸ“‹ ä½¿ç”¨è¯´æ˜:")
        logger.info(f"  - æ–°ç´¢å¼•å: {NEW_INDEX_NAME}")
        logger.info(f"  - åˆ«å: {ALIAS_NAME}")
        logger.info(f"  - åŸç´¢å¼•: {ORIGINAL_INDEX} (ä¿æŒä¸å˜)")
        logger.info("\nğŸ” æŸ¥è¯¢å­—æ®µ:")
        logger.info("  - content_jieba: jiebaåˆ†è¯çš„å†…å®¹å­—æ®µ")
        logger.info("  - content: åŸå§‹å†…å®¹å­—æ®µ")

        return True

    except Exception as e:
        logger.error(f"æ›´æ–°ç´¢å¼•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


def update_config_file():
    """æ›´æ–°é…ç½®æ–‡ä»¶ä»¥ä½¿ç”¨æ–°çš„ç´¢å¼•"""
    try:
        config_file = "/Users/caixiaomeng/Projects/Python/PerformanceRag/config/es_search_pipeline.yaml"

        # è¯»å–é…ç½®æ–‡ä»¶
        with open(config_file, "r", encoding="utf-8") as f:
            content = f.read()

        # æ›¿æ¢ç´¢å¼•å
        updated_content = content.replace(
            f'index_name: "{ORIGINAL_INDEX}"', f'index_name: "{ALIAS_NAME}"'
        )

        # å†™å›é…ç½®æ–‡ä»¶
        with open(config_file, "w", encoding="utf-8") as f:
            f.write(updated_content)

        logger.info(f"âœ… é…ç½®æ–‡ä»¶å·²æ›´æ–°ï¼Œç°åœ¨ä½¿ç”¨åˆ«å {ALIAS_NAME}")

    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")


if __name__ == "__main__":
    logger.info("=== å¼€å§‹æ›´æ–°ESç´¢å¼•ä»¥æ”¯æŒjiebaåˆ†è¯ ===")

    # æ£€æŸ¥jiebaæ˜¯å¦å®‰è£…
    try:
        import jieba

        logger.info("âœ… jiebaå·²å®‰è£…")
    except ImportError:
        logger.error("âŒ è¯·å…ˆå®‰è£…jieba: pip install jieba")
        sys.exit(1)

    # æ‰§è¡Œæ›´æ–°
    success = update_index_with_jieba()

    if success:
        logger.info("\nğŸ‰ ç´¢å¼•æ›´æ–°æˆåŠŸï¼")

        # æ›´æ–°é…ç½®æ–‡ä»¶
        update_config_file()

        logger.info("\nğŸ“ ä¸‹ä¸€æ­¥æ“ä½œ:")
        logger.info("1. æµ‹è¯•æ£€ç´¢æ•ˆæœ")
        logger.info("2. ç¡®è®¤æ— é—®é¢˜åå¯åˆ é™¤åŸç´¢å¼•")
        logger.info("3. åœ¨ä»£ç ä¸­ä½¿ç”¨ content_jieba å­—æ®µè¿›è¡Œjiebaåˆ†è¯æ£€ç´¢")

    else:
        logger.error("âŒ ç´¢å¼•æ›´æ–°å¤±è´¥")
        sys.exit(1)
