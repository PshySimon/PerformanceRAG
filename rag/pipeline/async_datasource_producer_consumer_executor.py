import asyncio
import time
from typing import Any, Dict, Iterator, List, Union

from utils.logger import get_logger

from .executor import PipelineExecutor


class AsyncDataSourceProducerConsumerPipelineExecutor:
    """å¼‚æ­¥æ•°æ®æº-ç”Ÿäº§è€…-æ¶ˆè´¹è€…Pipelineæ‰§è¡Œå™¨

    æ”¯æŒè¿­ä»£å™¨æ•°æ®æºï¼Œç”Ÿäº§è€…å¤šçº¿ç¨‹è¯»å–æ•°æ®æº
    """

    def __init__(self, config: Dict[str, Any]):
        self.logger = get_logger(__name__)
        self.config = config

        # åˆ›å»ºæ•°æ®æºã€ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…pipeline
        self.datasource_pipeline = PipelineExecutor(config["datasource"])
        self.producer_pipeline = PipelineExecutor(config["producer"])
        self.consumer_pipeline = PipelineExecutor(config["consumer"])

        # é˜Ÿåˆ—é…ç½®
        self.batch_size = config["consumer"]["config"]["batch_size"]
        self.max_queue_size = config["consumer"]["config"]["max_queue_size"]
        self.consumer_tasks = config["consumer"]["config"].get("consumer_tasks", 4)
        self.producer_tasks = config["consumer"]["config"].get("producer_tasks", 2)
        self.timeout = config["consumer"]["config"]["timeout"]

        # æµé‡æ§åˆ¶é…ç½®
        self.rate_limit_per_second = config["consumer"]["config"].get(
            "rate_limit_per_second", 3
        )
        self.rate_limit_window = 1.0

        # å¼‚æ­¥é˜Ÿåˆ—å’Œæ§åˆ¶å˜é‡
        self.data_queue = None
        self.producer_finished = None
        self.consumer_finished = None
        self.monitor_stop = None

        # ç”Ÿäº§è€…åè°ƒå˜é‡
        self.active_producers = 0
        self.producer_lock = None
        self.data_source_iterator = None
        self.data_source_lock = None

        # æµé‡æ§åˆ¶å˜é‡
        self.rate_limiter_lock = None
        self.request_timestamps = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_files_read = 0  # æ–°å¢ï¼šå·²è¯»å–æ–‡ä»¶æ•°
        self.total_data_read = 0
        self.total_data_produced = 0
        self.total_data_fetched = 0
        self.total_data_consumed = 0
        self.stats_lock = None

        # ç›‘æ§é…ç½®
        self.monitor_interval = 5
        self.start_time = None

        self.preload_all_data = config["consumer"]["config"].get(
            "preload_all_data", False
        )
        self.all_data_items = None
        self.total_data_count = 0

    async def _rate_limit_check(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œæµé‡æ§åˆ¶"""
        async with self.rate_limiter_lock:
            current_time = time.time()

            # æ¸…ç†è¿‡æœŸçš„æ—¶é—´æˆ³
            self.request_timestamps = [
                ts
                for ts in self.request_timestamps
                if current_time - ts < self.rate_limit_window
            ]

            # å¦‚æœå½“å‰æ—¶é—´çª—å£å†…çš„è¯·æ±‚æ•°å·²è¾¾åˆ°é™åˆ¶ï¼Œéœ€è¦ç­‰å¾…
            if len(self.request_timestamps) >= self.rate_limit_per_second:
                oldest_request = min(self.request_timestamps)
                wait_time = self.rate_limit_window - (current_time - oldest_request)

                if wait_time > 0:
                    self.logger.debug(
                        f"ğŸš¦ æµé‡æ§åˆ¶ï¼šç­‰å¾… {wait_time:.2f}s ä»¥é¿å…è¶…è¿‡é™é€Ÿ {self.rate_limit_per_second}/s"
                    )
                    await asyncio.sleep(wait_time)
                    current_time = time.time()

            # è®°å½•å½“å‰è¯·æ±‚æ—¶é—´æˆ³
            self.request_timestamps.append(current_time)

    def _create_data_source_iterator(
        self, input_data: Dict[str, Any]
    ) -> Iterator[Dict[str, Any]]:
        """åˆ›å»ºæ•°æ®æºè¿­ä»£å™¨"""
        try:
            # ç¡®ä¿æ•°æ®æºpipelineå·²æ„å»º
            if not self.datasource_pipeline._built:
                self.datasource_pipeline.build()

            # ä½¿ç”¨æ•°æ®æºpipelineçš„æµå¼æ‰§è¡Œ
            return self.datasource_pipeline.run_stream(input_data)

        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ•°æ®æºè¿­ä»£å™¨å¤±è´¥: {e}")
            raise

    async def _get_next_data_item(self) -> Union[Dict[str, Any], None]:
        """ä»æ•°æ®æºè¿­ä»£å™¨è·å–ä¸‹ä¸€ä¸ªæ•°æ®é¡¹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        async with self.data_source_lock:
            try:
                data_item = next(self.data_source_iterator)
                # ç»Ÿè®¡è¯»å–çš„æ–‡ä»¶æ•°
                async with self.stats_lock:
                    self.total_files_read += 1
                    self.total_data_read += len(data_item.get("documents", []))
                return data_item
            except StopIteration:
                return None
            except Exception as e:
                self.logger.error(f"ä»æ•°æ®æºè¯»å–æ•°æ®å¤±è´¥: {e}")
                return None

    async def _producer_worker(self, worker_id: int):
        """å¼‚æ­¥ç”Ÿäº§è€…å·¥ä½œä»»åŠ¡"""
        try:
            self.logger.info(
                f"ğŸš€ å¼‚æ­¥ç”Ÿäº§è€…ä»»åŠ¡ {worker_id} å¼€å§‹å·¥ä½œ...ï¼ˆæµé‡é™åˆ¶: {self.rate_limit_per_second}/sï¼‰"
            )

            # æ³¨å†Œæ´»è·ƒç”Ÿäº§è€…
            async with self.producer_lock:
                self.active_producers += 1

            # ç¡®ä¿ç”Ÿäº§è€…pipelineå·²æ„å»º
            if not self.producer_pipeline._built:
                self.producer_pipeline.build()

            # æŒç»­ä»æ•°æ®æºè¯»å–æ•°æ®
            while True:
                # ä»æ•°æ®æºè·å–ä¸‹ä¸€ä¸ªæ•°æ®é¡¹
                data_item = await self._get_next_data_item()
                if data_item is None:
                    break  # æ•°æ®æºå·²è€—å°½

                try:
                    # æ‰§è¡Œæµé‡æ§åˆ¶æ£€æŸ¥
                    await self._rate_limit_check()

                    # æ‰§è¡Œç”Ÿäº§è€…pipeline
                    loop = asyncio.get_event_loop()
                    results = await loop.run_in_executor(
                        None, lambda: list(self.producer_pipeline.run_stream(data_item))
                    )

                    # å°†ç»“æœæ”¾å…¥å¼‚æ­¥é˜Ÿåˆ—
                    for result in results:
                        await asyncio.wait_for(
                            self.data_queue.put(result), timeout=self.timeout
                        )

                        # ç»Ÿè®¡æ”¾å…¥é˜Ÿåˆ—çš„itemæ•°é‡
                        async with self.stats_lock:
                            self.total_data_produced += len(result.get("documents", []))

                except Exception as e:
                    import traceback

                    self.logger.error(
                        f"ç”Ÿäº§è€…ä»»åŠ¡ {worker_id} å¤„ç†æ•°æ®é¡¹å¤±è´¥: {type(e).__name__}: {str(e)}\n"
                        f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}"
                    )
                    continue

            self.logger.info(f"âœ… å¼‚æ­¥ç”Ÿäº§è€…ä»»åŠ¡ {worker_id} å®Œæˆ")

        except Exception as e:
            self.logger.error(f"âŒ å¼‚æ­¥ç”Ÿäº§è€…ä»»åŠ¡ {worker_id} å¼‚å¸¸: {e}")
            raise
        finally:
            # æ³¨é”€æ´»è·ƒç”Ÿäº§è€…
            async with self.producer_lock:
                self.active_producers -= 1
                if self.active_producers == 0:
                    self.producer_finished.set()
                    self.logger.info("âœ… æ‰€æœ‰å¼‚æ­¥ç”Ÿäº§è€…ä»»åŠ¡å·²å®Œæˆ")

    async def _consumer_worker(self, worker_id: int):
        """å¼‚æ­¥æ¶ˆè´¹è€…å·¥ä½œä»»åŠ¡"""
        try:
            self.logger.info(f"ğŸ”§ å¼‚æ­¥æ¶ˆè´¹è€…ä»»åŠ¡ {worker_id} å¼€å§‹å·¥ä½œ...")

            while True:
                batch_data = []

                # æ‰¹é‡ä»é˜Ÿåˆ—ä¸­å–æ•°æ®
                for _ in range(self.batch_size):
                    try:
                        item = await asyncio.wait_for(
                            self.data_queue.get(), timeout=self.timeout
                        )
                        batch_data.append(item)

                        # ç»Ÿè®¡å–å‡ºçš„æ•°æ®
                        async with self.stats_lock:
                            self.total_data_fetched += len(item.get("documents", []))

                    except asyncio.TimeoutError:
                        if self.producer_finished.is_set() and self.data_queue.empty():
                            if batch_data:
                                await self._process_batch(batch_data, worker_id)
                            return
                        break

                # å¤„ç†å½“å‰æ‰¹æ¬¡æ•°æ®
                if batch_data:
                    await self._process_batch(batch_data, worker_id)

                # æ£€æŸ¥é€€å‡ºæ¡ä»¶
                if self.producer_finished.is_set() and self.data_queue.empty():
                    return

        except Exception as e:
            self.logger.error(f"âŒ å¼‚æ­¥æ¶ˆè´¹è€…ä»»åŠ¡ {worker_id} å¼‚å¸¸: {e}")

    async def _process_batch(self, batch_data: List[Dict[str, Any]], worker_id: int):
        """å¼‚æ­¥å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        try:
            # åˆå¹¶æ‰¹æ¬¡æ•°æ®
            merged_documents = []
            for item in batch_data:
                if "documents" in item:
                    merged_documents.extend(item["documents"])

            if not merged_documents:
                return

            # æ„é€ æ¶ˆè´¹è€…è¾“å…¥
            consumer_input = {
                "documents": merged_documents,
                "batch_size": len(merged_documents),
                "worker_id": worker_id,
            }

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ¶ˆè´¹è€…pipeline
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.consumer_pipeline.run, consumer_input)

            # ç»Ÿè®¡æ¶ˆè´¹æ•°é‡
            async with self.stats_lock:
                self.total_data_consumed += sum(
                    len(item.get("documents", [])) for item in batch_data
                )

        except Exception as e:
            self.logger.error(f"âŒ å¼‚æ­¥æ¶ˆè´¹è€…ä»»åŠ¡ {worker_id} å¤„ç†æ‰¹æ¬¡å¼‚å¸¸: {e}")

    async def _progress_monitor(self):
        """å¼‚æ­¥å®æ—¶è¿›åº¦ç›‘æ§ä»»åŠ¡"""
        while not self.monitor_stop.is_set():
            if self.start_time:
                async with self.stats_lock:
                    current_time = time.time()
                    elapsed = current_time - self.start_time

                    # è®¡ç®—å¤„ç†é€Ÿåº¦
                    files_per_sec = (
                        self.total_files_read / elapsed if elapsed > 0 else 0
                    )
                    produced_per_sec = (
                        self.total_data_produced / elapsed if elapsed > 0 else 0
                    )
                    consume_per_sec = (
                        self.total_data_consumed / elapsed if elapsed > 0 else 0
                    )

                    processing_count = (
                        self.total_data_fetched - self.total_data_consumed
                    )

                    if hasattr(self, 'total_data_count') and self.total_data_count:
                        # é¢„åŠ è½½æ¨¡å¼ï¼šæ˜¾ç¤ºç²¾ç¡®è¿›åº¦å’Œå‰©ä½™æ—¶é—´ä¼°ç®—
                        progress = (self.total_files_read / self.total_data_count) * 100
                        remaining_files = self.total_data_count - self.total_files_read
                        
                        # ä¼°ç®—å‰©ä½™æ—¶é—´
                        if files_per_sec > 0:
                            eta_seconds = remaining_files / files_per_sec
                            eta_minutes = eta_seconds / 60
                            if eta_minutes > 60:
                                eta_str = f"{eta_minutes/60:.1f}h"
                            elif eta_minutes > 1:
                                eta_str = f"{eta_minutes:.1f}m"
                            else:
                                eta_str = f"{eta_seconds:.0f}s"
                        else:
                            eta_str = "æœªçŸ¥"
                        
                        if (
                            self.producer_finished.is_set()
                            and self.data_queue.qsize() == 0
                        ):
                            status = "æ”¶å°¾ä¸­"
                        else:
                            status = "å¤„ç†ä¸­"
                        
                        self.logger.info(
                            f"ğŸ“Š [{status}] è¿›åº¦: {progress:.1f}% ({self.total_files_read}/{self.total_data_count}) | "
                            f"é€Ÿåº¦: {files_per_sec:.1f}æ–‡ä»¶/s,ç”Ÿäº§{produced_per_sec:.1f}æ¡/s, æ¶ˆè´¹{consume_per_sec:.1f}æ¡/s | "
                            f"é˜Ÿåˆ—: {processing_count}å¾…å¤„ç† | ETA: {eta_str}"
                        )
                    else:
                        # æµå¼æ¨¡å¼ï¼šæ˜¾ç¤ºå·²å¤„ç†æ•°é‡å’Œé€Ÿåº¦
                        if (
                            self.producer_finished.is_set()
                            and self.data_queue.qsize() == 0
                        ):
                            status = "æ”¶å°¾ä¸­"
                        else:
                            status = "å¤„ç†ä¸­"
                        
                        self.logger.info(
                            f"ğŸ“Š [{status}] å·²å¤„ç†: {self.total_files_read} ä¸ªæ–‡ä»¶ | "
                            f"é€Ÿåº¦: {files_per_sec:.1f}æ–‡ä»¶/s, {consume_per_sec:.1f}æ¡/s | "
                            f"é˜Ÿåˆ—: {processing_count}å¾…å¤„ç† | è€—æ—¶: {elapsed:.1f}s"
                        )

            await asyncio.sleep(self.monitor_interval)

    def _preload_all_data(self, input_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜"""
        self.logger.info("ğŸ“‹ é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜...")

        # åˆ›å»ºæ•°æ®æºè¿­ä»£å™¨
        data_iterator = self._create_data_source_iterator(input_data)

        # å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜
        all_items = []
        for item in data_iterator:
            all_items.append(item)

        self.total_data_count = len(all_items)
        self.logger.info(f"ğŸ“‹ é¢„åŠ è½½å®Œæˆï¼Œæ€»æ•°æ®é‡: {self.total_data_count} ä¸ªæ•°æ®é¡¹")

        return all_items

    async def run_async(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰§è¡Œæ•°æ®æº-ç”Ÿäº§è€…-æ¶ˆè´¹è€…pipeline"""
        self.logger.info(
            f"ğŸš€ å¯åŠ¨å¼‚æ­¥æ•°æ®æº-ç”Ÿäº§è€…-æ¶ˆè´¹è€…Pipelineï¼ˆç”Ÿäº§è€…ä»»åŠ¡æ•°: {self.producer_tasks}ï¼Œæ¶ˆè´¹è€…ä»»åŠ¡æ•°: {self.consumer_tasks}ï¼Œæµé‡é™åˆ¶: {self.rate_limit_per_second}/sï¼‰..."
        )
        self.start_time = time.time()

        # åˆå§‹åŒ–å¼‚æ­¥æ§åˆ¶å˜é‡
        self.data_queue = asyncio.Queue(maxsize=self.max_queue_size)
        self.producer_finished = asyncio.Event()
        self.consumer_finished = asyncio.Event()
        self.monitor_stop = asyncio.Event()
        self.producer_lock = asyncio.Lock()
        self.data_source_lock = asyncio.Lock()
        self.stats_lock = asyncio.Lock()
        self.rate_limiter_lock = asyncio.Lock()

        try:
            # åˆ›å»ºæ•°æ®æºè¿­ä»£å™¨
            self.logger.info("ğŸ“‹ åˆ›å»ºæ•°æ®æºè¿­ä»£å™¨...")
            if self.preload_all_data:
                # é¢„åŠ è½½æ¨¡å¼ï¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
                self.all_data_items = self._preload_all_data(input_data)
                self.data_source_iterator = iter(self.all_data_items)
            else:
                # æµå¼æ¨¡å¼ï¼šä½¿ç”¨è¿­ä»£å™¨
                self.data_source_iterator = self._create_data_source_iterator(
                    input_data
                )
                self.total_data_count = None
            # åˆ›å»ºæ‰€æœ‰å¼‚æ­¥ä»»åŠ¡
            tasks = []

            # å¯åŠ¨è¿›åº¦ç›‘æ§ä»»åŠ¡
            monitor_task = asyncio.create_task(self._progress_monitor())
            tasks.append(monitor_task)

            # å¯åŠ¨ç”Ÿäº§è€…ä»»åŠ¡
            producer_tasks = []
            for i in range(self.producer_tasks):
                task = asyncio.create_task(self._producer_worker(i))
                producer_tasks.append(task)
                tasks.append(task)
                self.logger.info(f"ğŸ“¤ å¯åŠ¨å¼‚æ­¥ç”Ÿäº§è€…ä»»åŠ¡ {i}")

            # å¯åŠ¨æ¶ˆè´¹è€…ä»»åŠ¡
            consumer_tasks = []
            for i in range(self.consumer_tasks):
                task = asyncio.create_task(self._consumer_worker(i))
                consumer_tasks.append(task)
                tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰ç”Ÿäº§è€…å®Œæˆ
            await asyncio.gather(*producer_tasks)
            self.logger.info("âœ… æ‰€æœ‰å¼‚æ­¥ç”Ÿäº§è€…ä»»åŠ¡å·²å®Œæˆ")

            # ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹è€…å®Œæˆ
            await asyncio.gather(*consumer_tasks)
            self.logger.info("âœ… æ‰€æœ‰å¼‚æ­¥æ¶ˆè´¹è€…ä»»åŠ¡å·²å®Œæˆ")

        finally:
            # åœæ­¢ç›‘æ§ä»»åŠ¡
            self.monitor_stop.set()
            if "monitor_task" in locals():
                await monitor_task

            # æœ€ç»ˆç»Ÿè®¡
            total_time = time.time() - self.start_time
            avg_files_per_sec = (
                self.total_files_read / total_time if total_time > 0 else 0
            )
            avg_read_per_sec = (
                self.total_data_read / total_time if total_time > 0 else 0
            )
            avg_produce_per_sec = (
                self.total_data_produced / total_time if total_time > 0 else 0
            )
            avg_consume_per_sec = (
                self.total_data_consumed / total_time if total_time > 0 else 0
            )

            self.logger.info(
                f"ğŸ¯ å¼‚æ­¥æ•°æ®æºPipelineæ‰§è¡Œå®Œæˆï¼\n"
                f"   ğŸ“ è¯»å–æ–‡ä»¶: {self.total_files_read} ä¸ª (å¹³å‡ {avg_files_per_sec:.1f}/s)\n"
                f"   ğŸ“– è¯»å–æ•°æ®: {self.total_data_read} æ¡ (å¹³å‡ {avg_read_per_sec:.1f}/s)\n"
                f"   ğŸ“¤ ç”Ÿäº§æ•°æ®: {self.total_data_produced} æ¡ (å¹³å‡ {avg_produce_per_sec:.1f}/s)\n"
                f"   ğŸ“¥ æ¶ˆè´¹æ•°æ®: {self.total_data_consumed} æ¡ (å¹³å‡ {avg_consume_per_sec:.1f}/s)\n"
                f"   â±ï¸  æ€»è€—æ—¶: {total_time:.1f}s"
            )

        return {
            "status": "completed",
            "total_files_read": self.total_files_read,
            "total_data_read": self.total_data_read,
            "total_data_produced": self.total_data_produced,
            "total_data_consumed": self.total_data_consumed,
            "total_time": total_time,
        }

    def run(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥æ¥å£ï¼Œå†…éƒ¨è°ƒç”¨å¼‚æ­¥å®ç°"""
        return asyncio.run(self.run_async(input_data))
