import math
import queue
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List

from utils.logger import get_logger

from .executor import PipelineExecutor


class ProducerConsumerPipelineExecutor:
    """ç”Ÿäº§è€…-æ¶ˆè´¹è€…Pipelineæ‰§è¡Œå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.logger = get_logger(__name__)
        self.config = config

        # åˆ›å»ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…pipeline
        self.producer_pipeline = PipelineExecutor(config["producer"])
        self.consumer_pipeline = PipelineExecutor(config["consumer"])

        # é˜Ÿåˆ—é…ç½®
        self.batch_size = config["consumer"]["config"]["batch_size"]
        self.max_queue_size = config["consumer"]["config"]["max_queue_size"]
        self.consumer_threads = config["consumer"]["config"]["consumer_threads"]
        # æ–°å¢ï¼šç”Ÿäº§è€…çº¿ç¨‹æ•°é…ç½®
        self.producer_threads = config["consumer"]["config"].get("producer_threads", 1)
        self.timeout = config["consumer"]["config"]["timeout"]

        # åˆ›å»ºé˜Ÿåˆ—å’Œæ§åˆ¶å˜é‡
        self.data_queue = queue.Queue(maxsize=self.max_queue_size)
        self.producer_finished = threading.Event()
        self.consumer_finished = threading.Event()
        self.monitor_stop = threading.Event()

        # æ–°å¢ï¼šç”Ÿäº§è€…åè°ƒå˜é‡
        self.active_producers = 0
        self.producer_lock = threading.Lock()
        self.data_source = None  # å­˜å‚¨æ•°æ®æºç”¨äºåˆ†ç‰‡

        # ç»Ÿè®¡ä¿¡æ¯ - æ˜ç¡®åŒºåˆ†è¯»å–ã€ç”Ÿäº§ã€æ¶ˆè´¹
        self.total_data_read = 0  # å·²è¯»å–æ•°æ®æ¡æ•°ï¼ˆä»æ•°æ®æºè¯»å–çš„åŸå§‹æ•°æ®ï¼‰
        self.total_data_produced = 0  # å·²ç”Ÿäº§æ•°æ®æ¡æ•°ï¼ˆç”Ÿäº§è€…å¤„ç†åæ”¾å…¥é˜Ÿåˆ—çš„æ•°æ®ï¼‰
        self.total_data_fetched = 0  # å·²å–å‡ºæ•°æ®æ¡æ•°ï¼ˆæ¶ˆè´¹è€…ä»é˜Ÿåˆ—å–å‡ºçš„æ•°æ®ï¼‰
        self.total_data_consumed = 0  # å·²æ¶ˆè´¹æ•°æ®æ¡æ•°ï¼ˆæ¶ˆè´¹è€…å¤„ç†å®Œæˆçš„æ•°æ®ï¼‰
        self.stats_lock = threading.Lock()

        # ç›‘æ§é…ç½®
        self.monitor_interval = 5  # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        self.start_time = None

    def _get_data_source_items(self, input_data: Dict[str, Any]) -> List[Any]:
        """è·å–æ•°æ®æºçš„æ‰€æœ‰é¡¹ç›®ï¼Œç”¨äºåˆ†ç‰‡"""
        try:
            # ç¡®ä¿pipelineå·²æ„å»º
            if not self.producer_pipeline._built:
                self.producer_pipeline.build()

            # ä»å…¥å£ç»„ä»¶è·å–æ•°æ®æº
            if not self.producer_pipeline.entry_point:
                raise RuntimeError("æ— æ³•è·å–ç”Ÿäº§è€…pipelineçš„å…¥å£ç»„ä»¶")

            # è·å–æ•°æ®æºé¡¹ç›®åˆ—è¡¨ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„loaderå®ç°æ¥è°ƒæ•´ï¼‰
            entry_component = self.producer_pipeline.entry_point

            # å¦‚æœæ˜¯æ–‡ä»¶loaderï¼Œè·å–æ–‡ä»¶åˆ—è¡¨
            if hasattr(entry_component, "get_file_list"):
                return entry_component.get_file_list(input_data)
            elif hasattr(entry_component, "get_data_items"):
                return entry_component.get_data_items(input_data)
            else:
                # å›é€€æ–¹æ¡ˆï¼šé€šè¿‡get_data_lengthä¼°ç®—
                total_count = entry_component.get_data_length(input_data)
                return list(range(total_count))  # ä½¿ç”¨ç´¢å¼•ä½œä¸ºåˆ†ç‰‡ä¾æ®

        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®æºé¡¹ç›®å¤±è´¥: {e}")
            raise

    def _create_data_chunks(
        self, data_items: List[Any], num_chunks: int
    ) -> List[List[Any]]:
        """å°†æ•°æ®æºåˆ†ç‰‡ç»™å¤šä¸ªç”Ÿäº§è€…çº¿ç¨‹"""
        if not data_items:
            return []

        chunk_size = math.ceil(len(data_items) / num_chunks)
        chunks = []

        for i in range(0, len(data_items), chunk_size):
            chunk = data_items[i : i + chunk_size]
            if chunk:  # ç¡®ä¿chunkä¸ä¸ºç©º
                chunks.append(chunk)

        return chunks

    def _producer_worker(
        self, worker_id: int, data_chunk: List[Any], input_data: Dict[str, Any]
    ):
        """ç”Ÿäº§è€…å·¥ä½œçº¿ç¨‹ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
        try:
            self.logger.info(
                f"ğŸš€ ç”Ÿäº§è€…çº¿ç¨‹ {worker_id} å¼€å§‹å·¥ä½œï¼Œå¤„ç† {len(data_chunk)} ä¸ªæ•°æ®é¡¹..."
            )

            # æ³¨å†Œæ´»è·ƒç”Ÿäº§è€…
            with self.producer_lock:
                self.active_producers += 1

            # ç¡®ä¿pipelineå·²æ„å»º
            if not self.producer_pipeline._built:
                self.producer_pipeline.build()

            # ä¸ºæ¯ä¸ªæ•°æ®é¡¹åˆ›å»ºç‹¬ç«‹çš„è¾“å…¥
            for data_item in data_chunk:
                try:
                    # æ ¹æ®æ•°æ®é¡¹ç±»å‹åˆ›å»ºè¾“å…¥
                    if isinstance(data_item, str):  # æ–‡ä»¶è·¯å¾„
                        item_input = {**input_data, "path": data_item}
                    elif isinstance(data_item, dict):  # å·²ç»æ˜¯è¾“å…¥æ•°æ®
                        item_input = data_item
                    else:  # å…¶ä»–ç±»å‹ï¼Œä½¿ç”¨ç´¢å¼•
                        item_input = {**input_data, "item_index": data_item}

                    # æ‰§è¡Œç”Ÿäº§è€…pipelineå¹¶ç»Ÿè®¡æ”¾å…¥é˜Ÿåˆ—çš„itemæ•°é‡
                    for result in self.producer_pipeline.run_stream(item_input):
                        # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
                        self.data_queue.put(result, timeout=self.timeout)

                        # ç»Ÿè®¡æ”¾å…¥é˜Ÿåˆ—çš„itemæ•°é‡
                        with self.stats_lock:
                            self.total_data_produced += 1

                except Exception as e:
                    self.logger.error(f"ç”Ÿäº§è€…çº¿ç¨‹ {worker_id} å¤„ç†æ•°æ®é¡¹å¤±è´¥: {e}")
                    continue

            self.logger.info(f"âœ… ç”Ÿäº§è€…çº¿ç¨‹ {worker_id} å®Œæˆ")

        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿäº§è€…çº¿ç¨‹ {worker_id} å¼‚å¸¸: {e}")
            raise
        finally:
            # æ³¨é”€æ´»è·ƒç”Ÿäº§è€…
            with self.producer_lock:
                self.active_producers -= 1
                if self.active_producers == 0:
                    self.producer_finished.set()
                    self.logger.info("âœ… æ‰€æœ‰ç”Ÿäº§è€…çº¿ç¨‹å·²å®Œæˆ")

    def _consumer_worker(self, worker_id: int):
        """æ¶ˆè´¹è€…å·¥ä½œçº¿ç¨‹"""
        try:
            self.logger.info(f"ğŸ”§ æ¶ˆè´¹è€…çº¿ç¨‹ {worker_id} å¼€å§‹å·¥ä½œ...")

            while True:
                batch_data = []

                # æ‰¹é‡ä»é˜Ÿåˆ—ä¸­å–æ•°æ®
                for _ in range(self.batch_size):
                    try:
                        item = self.data_queue.get(timeout=self.timeout)
                        batch_data.append(item)

                        # ç»Ÿè®¡å–å‡ºçš„æ•°æ®
                        with self.stats_lock:
                            self.total_data_fetched += 1

                    except queue.Empty:
                        if self.producer_finished.is_set() and self.data_queue.empty():
                            if batch_data:
                                self._process_batch(batch_data, worker_id)
                            return
                        break

                # å¤„ç†å½“å‰æ‰¹æ¬¡æ•°æ®
                if batch_data:
                    self._process_batch(batch_data, worker_id)

                # æ£€æŸ¥é€€å‡ºæ¡ä»¶
                if self.producer_finished.is_set() and self.data_queue.empty():
                    return

        except Exception as e:
            self.logger.error(f"âŒ æ¶ˆè´¹è€…çº¿ç¨‹ {worker_id} å¼‚å¸¸: {e}")

    def _process_batch(self, batch_data: List[Dict[str, Any]], worker_id: int):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        try:
            # åˆå¹¶æ‰¹æ¬¡æ•°æ®
            merged_documents = []
            for item in batch_data:
                if "documents" in item:
                    merged_documents.extend(item["documents"])

            if not merged_documents:
                return

            # è®°å½•å¼€å§‹å¤„ç†çš„æ–‡æ¡£æ•°
            doc_count = len(merged_documents)

            # æ„é€ æ¶ˆè´¹è€…è¾“å…¥
            consumer_input = {
                "documents": merged_documents,
                "batch_size": doc_count,
                "worker_id": worker_id,
            }

            # æ‰§è¡Œæ¶ˆè´¹è€…pipeline
            self.consumer_pipeline.run(consumer_input)

            # åªæœ‰åœ¨çœŸæ­£å¤„ç†å®Œæˆåæ‰ç»Ÿè®¡æ¶ˆè´¹æ•°é‡
            with self.stats_lock:
                self.total_data_consumed += len(batch_data)

        except Exception as e:
            self.logger.error(f"âŒ æ¶ˆè´¹è€…çº¿ç¨‹ {worker_id} å¤„ç†æ‰¹æ¬¡å¼‚å¸¸: {e}")

    def _progress_monitor(self):
        """å®æ—¶è¿›åº¦ç›‘æ§çº¿ç¨‹"""
        while not self.monitor_stop.is_set():
            if self.start_time:
                with self.stats_lock:
                    current_time = time.time()
                    elapsed = current_time - self.start_time

                    # è®¡ç®—å¤„ç†é€Ÿåº¦
                    produced_per_sec = (
                        self.total_data_produced / elapsed if elapsed > 0 else 0
                    )
                    fetched_per_sec = (
                        self.total_data_fetched / elapsed if elapsed > 0 else 0
                    )
                    consume_per_sec = (
                        self.total_data_consumed / elapsed if elapsed > 0 else 0
                    )

                    processing_count = (
                        self.total_data_fetched - self.total_data_consumed
                    )

                    if self.total_data_consumed > 0:
                        if (
                            self.producer_finished.is_set()
                            and self.data_queue.qsize() == 0
                        ):
                            status = "æ”¶å°¾ä¸­"
                        else:
                            status = "å¤„ç†ä¸­"

                        self.logger.info(
                            f"ğŸ“Š {status} | æ•°æ®æº: {self.total_data_read}"
                            f"| å·²ç”Ÿäº§: {self.total_data_produced} ({produced_per_sec:.1f}/s) "
                            f"| å·²å–å‡º: {self.total_data_fetched} ({fetched_per_sec:.1f}/s) "
                            f"| å¤„ç†ä¸­: {processing_count} "
                            f"| å·²å®Œæˆ: {self.total_data_consumed} ({consume_per_sec:.1f}/s) "
                            f"| é˜Ÿåˆ—: {self.data_queue.qsize()}"
                        )

            self.monitor_stop.wait(self.monitor_interval)

    def run(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç”Ÿäº§è€…-æ¶ˆè´¹è€…pipeline"""
        self.logger.info(
            f"ğŸš€ å¯åŠ¨ç”Ÿäº§è€…-æ¶ˆè´¹è€…Pipelineï¼ˆç”Ÿäº§è€…çº¿ç¨‹æ•°: {self.producer_threads}ï¼‰..."
        )
        self.start_time = time.time()
        monitor_thread = None  # åˆå§‹åŒ–monitor_threadå˜é‡

        try:
            # è·å–æ•°æ®æºå¹¶åˆ†ç‰‡
            self.logger.info("ğŸ“‹ å‡†å¤‡æ•°æ®æºåˆ†ç‰‡...")
            data_items = self._get_data_source_items(input_data)

            with self.stats_lock:
                self.total_data_read = len(data_items)

            self.logger.info(
                f"ğŸ“‹ æ•°æ®æºæ€»é‡: {len(data_items)}ï¼Œå°†åˆ†é…ç»™ {self.producer_threads} ä¸ªç”Ÿäº§è€…çº¿ç¨‹"
            )

            # åˆ›å»ºæ•°æ®åˆ†ç‰‡
            data_chunks = self._create_data_chunks(data_items, self.producer_threads)

            if not data_chunks:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†")
                return {
                    "status": "completed",
                    "total_data_read": 0,
                    "total_data_produced": 0,
                    "total_data_consumed": 0,
                    "total_time": 0,
                }

            # å¯åŠ¨å®æ—¶è¿›åº¦ç›‘æ§çº¿ç¨‹
            monitor_thread = threading.Thread(
                target=self._progress_monitor, name="ProgressMonitor"
            )
            monitor_thread.start()

            # å¯åŠ¨å¤šä¸ªç”Ÿäº§è€…çº¿ç¨‹
            with ThreadPoolExecutor(
                max_workers=self.producer_threads, thread_name_prefix="Producer"
            ) as producer_executor:
                producer_futures = []

                for i, chunk in enumerate(data_chunks):
                    if chunk:  # ç¡®ä¿chunkä¸ä¸ºç©º
                        future = producer_executor.submit(
                            self._producer_worker, i, chunk, input_data
                        )
                        producer_futures.append(future)
                        self.logger.info(
                            f"ğŸ“¤ å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹ {i}ï¼Œå¤„ç† {len(chunk)} ä¸ªæ•°æ®é¡¹"
                        )

                # å¯åŠ¨æ¶ˆè´¹è€…çº¿ç¨‹æ± 
                with ThreadPoolExecutor(
                    max_workers=self.consumer_threads, thread_name_prefix="Consumer"
                ) as consumer_executor:
                    consumer_futures = []
                    for i in range(self.consumer_threads):
                        future = consumer_executor.submit(self._consumer_worker, i)
                        consumer_futures.append(future)

                    # ç­‰å¾…æ‰€æœ‰ç”Ÿäº§è€…å®Œæˆ
                    for future in producer_futures:
                        future.result()

                    self.logger.info("âœ… æ‰€æœ‰ç”Ÿäº§è€…çº¿ç¨‹å·²å®Œæˆ")

                    # ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹è€…å®Œæˆ
                    for future in consumer_futures:
                        future.result()

                    self.logger.info("âœ… æ‰€æœ‰æ¶ˆè´¹è€…çº¿ç¨‹å·²å®Œæˆ")

        finally:
            # ç­‰å¾…æ¶ˆè´¹è€…å®Œå…¨å®Œæˆåå†åœæ­¢ç›‘æ§
            time.sleep(0.1)  # ç»™æ¶ˆè´¹è€…ä¸€ç‚¹æ—¶é—´å®Œæˆæœ€åçš„ç»Ÿè®¡

            # æœ€åä¸€æ¬¡è¿›åº¦æ›´æ–°
            with self.stats_lock:
                current_time = time.time()
                elapsed = current_time - self.start_time
                consume_per_sec = (
                    self.total_data_consumed / elapsed if elapsed > 0 else 0
                )

                self.logger.info(
                    f"ğŸ“Š æ•°æ®æº: {self.total_data_read} "
                    f"| å·²ç”Ÿäº§æ•°æ®: {self.total_data_produced} "
                    f"| å·²æ¶ˆè´¹æ•°æ®: {self.total_data_consumed} ({consume_per_sec:.1f}/s) "
                    f"| é˜Ÿåˆ—: {self.data_queue.qsize()}"
                )

            # åœæ­¢ç›‘æ§çº¿ç¨‹
            self.monitor_stop.set()
            if monitor_thread is not None:  # æ·»åŠ ç©ºå€¼æ£€æŸ¥
                monitor_thread.join()

            # æœ€ç»ˆç»Ÿè®¡
            total_time = time.time() - self.start_time
            avg_read_per_sec = (
                self.total_data_read / total_time if total_time > 0 else 0
            )
            avg_produce_per_sec = (
                self.total_data_produced / total_time if total_time > 0 else 0
            )
            avg_consume_per_sec = (
                self.total_data_consumed / total_time if total_time > 0 else 0
            )

            self.logger.info(
                f"ğŸ¯ Pipelineæ‰§è¡Œå®Œæˆï¼\n"
                f"   ğŸ“– è¯»å–æ•°æ®: {self.total_data_read} æ¡ (å¹³å‡ {avg_read_per_sec:.1f}/s)\n"
                f"   ğŸ“¤ ç”Ÿäº§æ•°æ®: {self.total_data_produced} æ¡ (å¹³å‡ {avg_produce_per_sec:.1f}/s)\n"
                f"   ğŸ“¥ æ¶ˆè´¹æ•°æ®: {self.total_data_consumed} æ¡ (å¹³å‡ {avg_consume_per_sec:.1f}/s)\n"
                f"   â±ï¸  æ€»è€—æ—¶: {total_time:.1f}s"
            )

        return {
            "status": "completed",
            "total_data_read": self.total_data_read,
            "total_data_produced": self.total_data_produced,
            "total_data_consumed": self.total_data_consumed,
            "total_time": total_time,
        }
