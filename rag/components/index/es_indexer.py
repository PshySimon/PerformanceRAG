from typing import Any, Dict, List, Optional

import urllib3
from elasticsearch.helpers import bulk

from .base_indexer import BaseIndexer

# ç¦ç”¨ urllib3 çš„ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    from elasticsearch import Elasticsearch
    from elasticsearch.exceptions import NotFoundError, RequestError
except ImportError:
    Elasticsearch = None
    NotFoundError = Exception
    RequestError = Exception


class ESIndexerComponent(BaseIndexer):
    """Elasticsearchç´¢å¼•å™¨ç»„ä»¶"""

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)

        if Elasticsearch is None:
            raise ImportError("è¯·å®‰è£…elasticsearchåŒ…: pip install elasticsearch")

        # ESé…ç½®
        self.host = config.get("host", "localhost")
        self.port = config.get("port", 9200)
        self.username = config.get("username")
        self.password = config.get("password")
        self.use_ssl = config.get("use_ssl", False)
        self.verify_certs = config.get("verify_certs", False)

        # ç´¢å¼•é…ç½®
        self.mapping = config.get("mapping", self._default_mapping())
        self.settings = config.get("settings", self._default_settings())

        self.client = None

        # ç§»é™¤æµå¼å¤„ç†ç›¸å…³çš„ç¼“å†²åŒº
        # self._document_buffer = []
        # self._total_indexed = 0

        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.disable_refresh = config.get("disable_refresh", True)  # ç¦ç”¨å®æ—¶åˆ·æ–°
        self.bulk_timeout = config.get("bulk_timeout", 60)
        self.max_chunk_bytes = config.get("max_chunk_bytes", 15 * 1024 * 1024)  # 15MB

    def _do_initialize(self):
        """åˆå§‹åŒ–ESå®¢æˆ·ç«¯"""
        try:
            # æ„å»ºè¿æ¥é…ç½®
            if self.use_ssl:
                # å¯¹äº ES 8.xï¼Œä½¿ç”¨ https scheme
                hosts = [f"https://{self.host}:{self.port}"]
            else:
                hosts = [f"http://{self.host}:{self.port}"]

            # ä¼˜åŒ–çš„ESé…ç½®
            es_config = {
                "hosts": hosts,
                "verify_certs": self.verify_certs,
                "timeout": self.bulk_timeout,
                "max_retries": 3,
                "retry_on_timeout": True,
                # è¿æ¥æ± ä¼˜åŒ–
                "maxsize": 25,
                "http_compress": True,  # å¯ç”¨å‹ç¼©
            }

            if self.username and self.password:
                es_config["basic_auth"] = (self.username, self.password)

            self.client = Elasticsearch(**es_config)

            # æµ‹è¯•è¿æ¥
            if not self.client.ping():
                raise ConnectionError("æ— æ³•è¿æ¥åˆ°Elasticsearch")

            if self.debug:
                self.logger.debug(f"æˆåŠŸè¿æ¥åˆ°Elasticsearch: {self.host}:{self.port}")

        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–Elasticsearchå®¢æˆ·ç«¯å¤±è´¥: {e}")
            raise

    def _default_mapping(self) -> Dict[str, Any]:
        """é»˜è®¤æ˜ å°„é…ç½®"""
        return {
            "properties": {
                "content": {"type": "text", "analyzer": "standard"},
                "metadata": {"type": "object", "enabled": True},
                "timestamp": {"type": "date"},
            }
        }

    def _default_settings(self) -> Dict[str, Any]:
        """é»˜è®¤è®¾ç½®é…ç½®"""
        return {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {"analyzer": {"default": {"type": "standard"}}},
        }

    def create_index(self, index_name: Optional[str] = None, **kwargs) -> bool:
        """åˆ›å»ºç´¢å¼•"""
        target_index = index_name or self.index_name
        try:
            if self.client.indices.exists(index=target_index):
                if self.debug:
                    self.logger.debug(f"ç´¢å¼• {target_index} å·²å­˜åœ¨")
                return True
    
            body = {"mappings": self.mapping, "settings": self.settings}
            self.client.indices.create(index=target_index, body=body)
    
            if self.debug:
                self.logger.debug(f"æˆåŠŸåˆ›å»ºç´¢å¼•: {target_index}")
            return True
    
        except RequestError as e:
            # å¤„ç†ç´¢å¼•å·²å­˜åœ¨çš„æƒ…å†µ
            if e.error == 'resource_already_exists_exception':
                if self.debug:
                    self.logger.debug(f"ç´¢å¼• {target_index} å·²å­˜åœ¨ï¼ˆå¹¶å‘åˆ›å»ºï¼‰")
                return True
            else:
                self.logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
                return False
        except Exception as e:
            self.logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            return False

    # ç§»é™¤ delete_index æ–¹æ³• - å¤ªå±é™©äº†ï¼
    # def delete_index(self) -> bool:
    #     """åˆ é™¤ç´¢å¼• - å·²ç§»é™¤ï¼Œå¤ªå±é™©"""
    #     pass

    def index_documents(
        self, documents: List[Dict[str, Any]], index_name: Optional[str] = None
    ) -> bool:
        """ä¼˜åŒ–çš„æ‰¹é‡ç´¢å¼•æ–‡æ¡£"""
        target_index = index_name or self.index_name
        try:
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            self.create_index(target_index)
    
            if self.debug:
                self.logger.debug(f"å¼€å§‹ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£åˆ° {target_index}")
    
            # å‡†å¤‡æ‰¹é‡æ“ä½œ
            actions = []
            for i, doc in enumerate(documents):
                # ğŸ”§ ä¿®å¤ï¼šç”Ÿæˆå…¨å±€å”¯ä¸€ID
                if "id" not in doc:
                    # å°è¯•ä»metadataä¸­è·å–node_id
                    if "metadata" in doc and "node_id" in doc["metadata"]:
                        doc_id = doc["metadata"]["node_id"]
                    else:
                        # ä½¿ç”¨æ—¶é—´æˆ³å’Œéšæœºæ•°ç”Ÿæˆå”¯ä¸€ID
                        import time
                        import uuid
                        doc_id = f"doc_{int(time.time() * 1000000)}_{uuid.uuid4().hex[:8]}"
                else:
                    doc_id = doc["id"]
                    
                action = {"_index": target_index, "_id": doc_id, "_source": doc}
                actions.append(action)
    
            # ä¼˜åŒ–çš„æ‰¹é‡ç´¢å¼•
            try:
                success_count, failed_items = bulk(
                    self.client,
                    actions,
                    chunk_size=self.batch_size,
                    max_chunk_bytes=self.max_chunk_bytes,  # é™åˆ¶æ‰¹æ¬¡å¤§å°
                    request_timeout=self.bulk_timeout,
                    refresh=False,  # ç¦ç”¨ç«‹å³åˆ·æ–°
                )
            except Exception as bulk_error:
                self.logger.error(f"æ‰¹é‡ç´¢å¼•æ“ä½œå¤±è´¥: {bulk_error}")
                raise

            # ç§»é™¤è‡ªåŠ¨åˆ·æ–° - è®©ESè‡ªåŠ¨å¤„ç†
            # if self.debug:
            #     self.logger.debug("åˆ·æ–°ç´¢å¼•ä»¥ç¡®ä¿æ–‡æ¡£å¯æœç´¢")
            # self.client.indices.refresh(index=target_index)

            return len(failed_items) == 0

        except Exception as e:
            self.logger.error(f"ç´¢å¼•æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def search(
        self, query: str, top_k: int = 10, index_name: Optional[str] = None, **kwargs
    ) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£"""
        target_index = index_name or self.index_name
        try:
            # æ„å»ºæŸ¥è¯¢ - ä¿®å¤ï¼šæ˜ç¡®æŒ‡å®šæœç´¢å­—æ®µï¼Œé¿å…æœç´¢æ—¥æœŸå­—æ®µ
            # å¯ä»¥ä½¿ç”¨ bool æŸ¥è¯¢ç»„åˆå¤šä¸ªæ¡ä»¶
            search_body = {
                "query": {
                    "bool": {
                        "should": [
                            {"match": {"content": {"query": query, "boost": 2}}},
                            {
                                "match": {
                                    "metadata.title": {"query": query, "boost": 1.5}
                                }
                            },
                            {"match": {"metadata.category": query}},
                            {"match": {"metadata.tags": query}},
                        ]
                    }
                },
                "size": top_k,
            }

            # æ‰§è¡Œæœç´¢
            response = self.client.search(index=target_index, body=search_body)

            # å¤„ç†ç»“æœ
            results = []
            for hit in response["hits"]["hits"]:
                result = {
                    "id": hit["_id"],
                    "score": hit["_score"],
                    "content": hit["_source"].get("content", ""),
                    "metadata": hit["_source"].get("metadata", {}),
                }
                results.append(result)

            if self.debug:
                self.logger.debug(f"ä» {target_index} æœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")

            return results

        except Exception as e:
            self.logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []

    def get_document(
        self, doc_id: str, index_name: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–æ–‡æ¡£"""
        target_index = index_name or self.index_name
        try:
            response = self.client.get(index=target_index, id=doc_id)
            return {
                "id": response["_id"],
                "content": response["_source"].get("content", ""),
                "metadata": response["_source"].get("metadata", {}),
            }
        except NotFoundError:
            return None
        except Exception as e:
            self.logger.error(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
            return None

    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£æ•°æ®å¹¶è¿›è¡Œç´¢å¼•"""
        if "documents" not in data:
            error_msg = "è¾“å…¥æ•°æ®ä¸­ç¼ºå°‘ 'documents' å­—æ®µ"
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        documents = data["documents"]
        if not documents:
            self.logger.warning("æ²¡æœ‰æ–‡æ¡£éœ€è¦ç´¢å¼•")
            return data

        if self.debug:
            self.logger.debug(f"å¼€å§‹ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£")
            # æ·»åŠ æ–‡æ¡£å†…å®¹çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            total_content_length = sum(
                len(str(doc.get("content", ""))) for doc in documents
            )
            avg_content_length = (
                total_content_length / len(documents) if documents else 0
            )
            self.logger.debug(
                f"æ–‡æ¡£ç»Ÿè®¡ - æ€»æ•°: {len(documents)}, å¹³å‡å†…å®¹é•¿åº¦: {avg_content_length:.0f} å­—ç¬¦"
            )

        try:
            # æ‰¹é‡ç´¢å¼•æ–‡æ¡£
            success = self.index_documents(documents)

            if not success:
                self.logger.error(f"ç´¢å¼•æ–‡æ¡£å¤±è´¥ - å…± {len(documents)} ä¸ªæ–‡æ¡£")

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            self.logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            import traceback

            self.logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            success = False

        # è¿”å›å¤„ç†ç»“æœ
        result = data.copy()
        result["indexed"] = success
        result["document_count"] = len(documents)
        result["index_name"] = self.index_name
        result["metadata"] = {
            "component": self.name,
            "indexer_type": self.__class__.__name__,
            "batch_size": self.batch_size,
        }

        return result
