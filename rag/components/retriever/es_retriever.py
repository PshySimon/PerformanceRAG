from typing import Any, Dict, List, Optional

from elasticsearch import Elasticsearch

from utils.config import config

from ..embedding.embedding_factory import EmbeddingFactory
from .base_retriever import BaseRetrieverComponent


class ESRetrieverComponent(BaseRetrieverComponent):
    """Elasticsearchæ£€ç´¢å™¨ç»„ä»¶"""

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)

        if Elasticsearch is None:
            raise ImportError("è¯·å®‰è£…elasticsearchåŒ…: pip install elasticsearch")
            
        # ä¼˜å…ˆæ£€ç´¢çš„chunk_level
        self.preferred_chunk_level = config.get("preferred_chunk_level")

        # ESé…ç½®
        self.host = config.get("host", "localhost")
        self.port = config.get("port", 9200)
        self.username = config.get("username")
        self.password = config.get("password")
        self.use_ssl = config.get("use_ssl", False)
        self.verify_certs = config.get("verify_certs", False)

        # æ£€ç´¢é…ç½®
        self.search_type = config.get("search_type", "text")  # text, vector, hybrid
        self.embedding_config = config.get("embedding", {})
        
        # Small2Bigæ£€ç´¢é…ç½®
        self.enable_small2big = config.get("enable_small2big", False)
        small2big_config = config.get("small2big_config", {})
        self.small_chunk_top_k = small2big_config.get("small_chunk_top_k", 20)
        self.final_top_k = small2big_config.get("final_top_k", 10)
        self.expansion_strategy = small2big_config.get("expansion_strategy", "parent_expansion")
        self.similarity_threshold_small = small2big_config.get("similarity_threshold_small", 0.6)
        self.score_fusion_method = small2big_config.get("score_fusion_method", "max")
        self.enable_score_normalization = small2big_config.get("enable_score_normalization", True)
        self.diversity_threshold = small2big_config.get("diversity_threshold", 0.8)
        self.max_small_chunks_per_big = small2big_config.get("max_small_chunks_per_big", 5)
        
        # RRFé…ç½®
        self.fusion_method = config.get("fusion_method", "weighted")  # weighted, rrf
        self.rrf_k = config.get("rrf_k", 60)  # RRFå‚æ•°
        self.text_weight = config.get("text_weight", 1.0)  # æ–‡æœ¬æ£€ç´¢æƒé‡
        self.vector_weight = config.get("vector_weight", 1.2)  # å‘é‡æ£€ç´¢æƒé‡

        # æ·»åŠ è‡ªå®šä¹‰æŸ¥è¯¢å­—æ®µæ”¯æŒ
        self.search_fields = config.get(
            "search_fields",
            {
                "content": 2.0,
                "metadata.title": 1.5,
                "metadata.category": 1.0,
                "metadata.tags": 1.0,
            },
        )
        
        # æ·»åŠ åˆ†æå™¨é…ç½®è¯»å–
        self.analyzer_config = config.get("analyzer_config", {})
        self.search_analyzer = self.analyzer_config.get("search_analyzer", "ik_search_analyzer")
        self.index_analyzer = self.analyzer_config.get("index_analyzer", "ik_analyzer")
        
        # æ·»åŠ é«˜äº®é…ç½®è¯»å–
        self.highlight_config = config.get("highlight_config", {})
        self.default_highlight_settings = {
            "require_field_match": self.highlight_config.get("require_field_match", False),
            "fragment_size": self.highlight_config.get("fragment_size", 150),
            "number_of_fragments": self.highlight_config.get("number_of_fragments", 3),
            "pre_tags": self.highlight_config.get("pre_tags", ["<mark>"]),
            "post_tags": self.highlight_config.get("post_tags", ["</mark>"]),
        }

        # å†…éƒ¨çŠ¶æ€
        self.client = None
        self.embedding_client = None

    def _do_initialize(self):
        """åˆå§‹åŒ–ESå®¢æˆ·ç«¯å’Œembeddingå®¢æˆ·ç«¯"""
        try:
            self._connect_to_index()

            # å¦‚æœéœ€è¦å‘é‡æ£€ç´¢ï¼Œåˆå§‹åŒ–embeddingå®¢æˆ·ç«¯
            if self.search_type in ["vector", "hybrid"]:
                self._init_embedding_client()

            if self.debug:
                self.logger.debug(f"ESæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ£€ç´¢ç±»å‹: {self.search_type}, èåˆæ–¹æ³•: {self.fusion_method}")

        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–ESæ£€ç´¢å™¨å¤±è´¥: {e}")
            raise

    def _connect_to_index(self):
        """è¿æ¥åˆ°ESç´¢å¼•"""
        try:
            # æ„å»ºè¿æ¥é…ç½®
            if self.use_ssl:
                hosts = [f"https://{self.host}:{self.port}"]
            else:
                hosts = [f"http://{self.host}:{self.port}"]

            es_config = {
                "hosts": hosts,
                "verify_certs": self.verify_certs,
            }

            if self.username and self.password:
                es_config["basic_auth"] = (self.username, self.password)

            self.client = Elasticsearch(**es_config)

            # æµ‹è¯•è¿æ¥
            if not self.client.ping():
                raise ConnectionError("æ— æ³•è¿æ¥åˆ°Elasticsearch")

            if self.debug:
                self.logger.debug(f"æˆåŠŸè¿æ¥åˆ°Elasticsearch: {self.host}:{self.port}")

        except Exception as e:
            self.logger.error(f"è¿æ¥Elasticsearchå¤±è´¥: {e}")
            raise

    def _init_embedding_client(self):
        """åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯"""
        try:
            embedding_type = self.embedding_config.get("type", "hf")
            if embedding_type == "hf":
                emb_cfg = config.embeddings.clients.hf
                self.embedding_client = EmbeddingFactory.create(
                    "hf", model_name=emb_cfg.model_name
                )
            elif embedding_type == "openai":
                # ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
                self.embedding_client = EmbeddingFactory.create(
                    "openai",
                    model=self.embedding_config.get("model"),
                    api_key=self.embedding_config.get("api_key"),
                    api_base=self.embedding_config.get("api_base"),
                    batch_size=self.embedding_config.get("batch_size", 10),
                    dimensions=self.embedding_config.get("dimensions"),
                    timeout=self.embedding_config.get("timeout", 60),
                    max_retries=self.embedding_config.get("max_retries", 3),
                )
            elif embedding_type == "bge_embedding":
                emb_cfg = config.embeddings.clients.bge_embedding
                self.embedding_client = EmbeddingFactory.create(
                    "bge_embedding", api_url=emb_cfg.api_url
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„embeddingç±»å‹: {embedding_type}")

            if self.debug:
                self.logger.debug(f"Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {embedding_type}")

        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯å¤±è´¥: {e}")
            raise

    def retrieve(
        self, query: str, top_k: Optional[int] = None, **kwargs
    ) -> List[Dict[str, Any]]:
        """æ£€ç´¢æ–‡æ¡£"""
        use_top_k = top_k or self.top_k

        try:
            if self.debug:
                self.logger.debug(
                    f"ğŸ” å¼€å§‹ESæ£€ç´¢ - æŸ¥è¯¢: '{query}', æ£€ç´¢ç±»å‹: {self.search_type}, Top-K: {use_top_k}"
                )

            # å¦‚æœå¯ç”¨Small2Bigæ£€ç´¢
            if self.enable_small2big:
                return self._small2big_search(query, use_top_k)
            
            # åŸæœ‰æ£€ç´¢é€»è¾‘
            if self.search_type == "text":
                return self._text_search(query, use_top_k)
            elif self.search_type == "vector":
                return self._vector_search(query, use_top_k)
            elif self.search_type == "hybrid":
                return self._hybrid_search(query, use_top_k)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ£€ç´¢ç±»å‹: {self.search_type}")

        except Exception as e:
            import traceback

            self.logger.error(f"æ£€ç´¢å¤±è´¥: {e}\n{traceback.format_exc()}")
            return []

    def _small2big_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """Small2Bigæ£€ç´¢ç­–ç•¥å®ç°"""
        if self.debug:
            self.logger.info(f"ğŸ” å¼€å§‹Small2Bigæ£€ç´¢ - æŸ¥è¯¢: '{query}'")
            self.logger.info(f"   ç¬¬ä¸€é˜¶æ®µ: æ£€ç´¢Small Chunk (ç›®æ ‡æ•°é‡: {self.small_chunk_top_k})")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ£€ç´¢Small Chunk
        small_chunks = self._search_small_chunks(query, self.small_chunk_top_k)
        
        if not small_chunks:
            self.logger.warning("âš ï¸ Small Chunkæ£€ç´¢æ— ç»“æœ")
            return []
        
        if self.debug:
            self.logger.info(f"   âœ… Small Chunkæ£€ç´¢å®Œæˆï¼Œè·å¾— {len(small_chunks)} ä¸ªç»“æœ")
            self.logger.info(f"   ç¬¬äºŒé˜¶æ®µ: æ‰©å±•åˆ°Big Chunkå¹¶èåˆåˆ†æ•°")
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ ¹æ®ç­–ç•¥æ‰©å±•åˆ°Big Chunk
        big_chunks = self._expand_to_big_chunks(small_chunks)
        
        if self.debug:
            self.logger.info(f"   âœ… æ‰©å±•å®Œæˆï¼Œè·å¾— {len(big_chunks)} ä¸ªBig Chunk")
            self.logger.info(f"   ç¬¬ä¸‰é˜¶æ®µ: åˆ†æ•°èåˆå’Œé‡æ’åº")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†æ•°èåˆå’Œé‡æ’åº
        final_results = self._fuse_and_rerank(big_chunks, top_k)
        
        if self.debug:
            self.logger.info(f"ğŸ¯ Small2Bigæ£€ç´¢å®Œæˆï¼Œæœ€ç»ˆè¿”å› {len(final_results)} ä¸ªç»“æœ")
            for i, result in enumerate(final_results[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
                self.logger.debug(f"   #{i} ID: {result['id']}, åˆ†æ•°: {result['score']:.4f}, æ¥æºSmall Chunkæ•°: {result.get('source_small_chunks_count', 1)}")
        
        return final_results
    
    def _build_highlight_fields(self) -> Dict[str, Any]:
        """åŠ¨æ€æ„å»ºé«˜äº®å­—æ®µé…ç½®"""
        highlight_fields = {}
        
        # ä»é…ç½®ä¸­è·å–å­—æ®µç‰¹å®šçš„é«˜äº®è®¾ç½®
        field_configs = self.highlight_config.get("fields", {})
        
        # ä¸ºæ‰€æœ‰æœç´¢å­—æ®µæ·»åŠ é«˜äº®é…ç½®
        for field in self.search_fields.keys():
            if field in field_configs:
                # ä½¿ç”¨å­—æ®µç‰¹å®šé…ç½®
                field_config = field_configs[field].copy()
                highlight_fields[field] = field_config
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                highlight_fields[field] = {
                    "fragment_size": self.default_highlight_settings["fragment_size"],
                    "number_of_fragments": self.default_highlight_settings["number_of_fragments"],
                    "pre_tags": self.default_highlight_settings["pre_tags"],
                    "post_tags": self.default_highlight_settings["post_tags"]
                }
        
        # ç‰¹åˆ«å¤„ç† content_jieba å­—æ®µï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„åˆ†æå™¨
        if "content_jieba" in highlight_fields:
            highlight_fields["content_jieba"]["analyzer"] = self.search_analyzer
            
        if self.debug:
            self.logger.debug(f"ğŸ¨ æ„å»ºçš„é«˜äº®å­—æ®µé…ç½®: {highlight_fields}")
            
        return highlight_fields

    def _search_small_chunks(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ£€ç´¢Small Chunk - åŠ¨æ€æ£€æµ‹ç‰ˆæœ¬"""
        # åŠ¨æ€æ£€æµ‹chunk_levelå­—æ®µè·¯å¾„å’Œå€¼
        chunk_level_info = self._detect_chunk_level_config()
        
        if not chunk_level_info:
            self.logger.warning("æ— æ³•æ£€æµ‹chunk_levelé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            chunk_level_filter = {
                "term": {
                    "metadata.chunk_level": 2  # å›é€€åˆ°é»˜è®¤å€¼
                }
            }
        else:
            chunk_level_filter = {
                "term": {
                    chunk_level_info["field_path"]: chunk_level_info["small_chunk_level"]
                }
            }
        
        if self.debug:
            self.logger.debug(f"ğŸ” Small Chunkè¿‡æ»¤æ¡ä»¶: {chunk_level_filter}")

        
        if self.search_type == "hybrid":
            # æ··åˆæ£€ç´¢Small Chunk
            return self._hybrid_search_with_filter(query, top_k, chunk_level_filter)
        elif self.search_type == "vector":
            return self._vector_search_with_filter(query, top_k, chunk_level_filter)
        else:
            return self._text_search_with_filter(query, top_k, chunk_level_filter)

    def _expand_to_big_chunks(self, small_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†Small Chunkæ‰©å±•åˆ°Big Chunk"""
        big_chunk_groups = {}
        
        for small_chunk in small_chunks:
            metadata = small_chunk.get("metadata", {})
            
            if self.expansion_strategy == "parent_expansion":
                # é€šè¿‡parent_idæ‰©å±•
                parent_id = metadata.get("parent_id")
                if parent_id:
                    if parent_id not in big_chunk_groups:
                        big_chunk_groups[parent_id] = {
                            "small_chunks": [],
                            "max_score": 0,
                            "avg_score": 0,
                            "big_chunk_data": None
                        }
                    
                    big_chunk_groups[parent_id]["small_chunks"].append(small_chunk)
                    big_chunk_groups[parent_id]["max_score"] = max(
                        big_chunk_groups[parent_id]["max_score"], 
                        small_chunk["score"]
                    )
            
            elif self.expansion_strategy == "root_expansion":
                # é€šè¿‡root_idæ‰©å±•åˆ°æœ€å¤§ç²’åº¦
                root_id = metadata.get("root_id")
                if root_id:
                    if root_id not in big_chunk_groups:
                        big_chunk_groups[root_id] = {
                            "small_chunks": [],
                            "max_score": 0,
                            "avg_score": 0,
                            "big_chunk_data": None
                        }
                    
                    big_chunk_groups[root_id]["small_chunks"].append(small_chunk)
                    big_chunk_groups[root_id]["max_score"] = max(
                        big_chunk_groups[root_id]["max_score"], 
                        small_chunk["score"]
                    )
        
        # è·å–Big Chunkçš„å®Œæ•´å†…å®¹
        for big_chunk_id, group_data in big_chunk_groups.items():
            big_chunk_content = self._get_big_chunk_content(big_chunk_id)
            if big_chunk_content:
                group_data["big_chunk_data"] = big_chunk_content
                
                # è®¡ç®—å¹³å‡åˆ†æ•°
                scores = [chunk["score"] for chunk in group_data["small_chunks"]]
                group_data["avg_score"] = sum(scores) / len(scores)
        
        return big_chunk_groups

    def _get_big_chunk_content(self, big_chunk_id: str) -> Optional[Dict[str, Any]]:
        """è·å–Big Chunkçš„å®Œæ•´å†…å®¹"""
        try:
            # ä½¿ç”¨ .keyword å­—æ®µè¿›è¡Œç²¾ç¡®åŒ¹é…
            search_body = {
                "query": {
                    "term": {
                        "metadata.chunk_id.keyword": big_chunk_id
                    }
                },
                "size": 1
            }
            
            response = self.client.search(index=self.index_name, body=search_body)
            
            if response["hits"]["total"]["value"] > 0:
                hit = response["hits"]["hits"][0]
                return {
                    "id": hit["_id"],
                    "content": hit["_source"].get("content", ""),
                    "metadata": hit["_source"].get("metadata", {})
                }
        except Exception as e:
            if self.debug:
                self.logger.warning(f"è·å–Big Chunk {big_chunk_id} å¤±è´¥: {e}")
        return None

    def _fuse_and_rerank(self, big_chunk_groups: Dict, top_k: int) -> List[Dict[str, Any]]:
        """èåˆåˆ†æ•°å¹¶é‡æ–°æ’åº"""
        results = []
        
        for big_chunk_id, group_data in big_chunk_groups.items():
            if not group_data["big_chunk_data"]:
                continue
            
            # æ ¹æ®èåˆæ–¹æ³•è®¡ç®—æœ€ç»ˆåˆ†æ•°
            if self.score_fusion_method == "max":
                final_score = group_data["max_score"]
            elif self.score_fusion_method == "avg":
                final_score = group_data["avg_score"]
            elif self.score_fusion_method == "weighted_avg":
                # åŠ æƒå¹³å‡ï¼Œåˆ†æ•°è¶Šé«˜æƒé‡è¶Šå¤§
                scores = [chunk["score"] for chunk in group_data["small_chunks"]]
                weights = [score / sum(scores) for score in scores]
                final_score = sum(score * weight for score, weight in zip(scores, weights))
            else:
                final_score = group_data["max_score"]
            
            result = {
                "id": big_chunk_id,
                "score": final_score,
                "content": group_data["big_chunk_data"]["content"],
                "metadata": group_data["big_chunk_data"]["metadata"],
                "recall_source": "small2big",
                "source_small_chunks": group_data["small_chunks"],
                "source_small_chunks_count": len(group_data["small_chunks"]),
                "max_small_score": group_data["max_score"],
                "avg_small_score": group_data["avg_score"],
                "fusion_method": self.score_fusion_method
            }
            
            results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]

    def _hybrid_search_with_filter(self, query: str, top_k: int, filter_clause: Dict) -> List[Dict[str, Any]]:
        """å¸¦è¿‡æ»¤æ¡ä»¶çš„æ··åˆæ£€ç´¢"""
        if not self.embedding_client:
            raise RuntimeError("æ··åˆæ£€ç´¢éœ€è¦embeddingå®¢æˆ·ç«¯")
        
        vector_field = self.embedding_config.get("field_name", "embedding")
        query_embedding = self.embedding_client.embed_text(query)
        
        # æ–‡æœ¬æ£€ç´¢éƒ¨åˆ†
        should_queries = []
        for field, boost in self.search_fields.items():
            should_queries.append({"match": {field: {"query": query, "boost": boost, "analyzer": self.search_analyzer}}})
        
        filter_clauses = [filter_clause]
        if preferred_chunk_level := self.config.get("preferred_chunk_level"):
            filter_clauses.append({
                "term": {
                    "metadata.chunk_level": preferred_chunk_level
                }
            })
        
        text_search_body = {
            "query": {
                "bool": {
                    "should": should_queries,
                    "filter": filter_clauses
                }
            },
            "size": top_k * 2,
        }
        
        # å‘é‡æ£€ç´¢éƒ¨åˆ†
        vector_search_body = {
            "knn": {
                "field": vector_field,
                "query_vector": query_embedding,
                "k": top_k * 2,
                "num_candidates": top_k * 4,
                "filter": {
                    "bool": {
                        "filter": filter_clauses
                    }
                }
            },
            "size": top_k * 2,
        }
        
        text_response = self.client.search(index=self.index_name, body=text_search_body)
        vector_response = self.client.search(index=self.index_name, body=vector_search_body)
        
        # ä½¿ç”¨ç°æœ‰çš„èåˆæ–¹æ³•
        if self.fusion_method == "rrf":
            return self._merge_hybrid_results_with_rrf(text_response, vector_response, query, top_k)
        else:
            return self._merge_hybrid_results_with_highlights(text_response, vector_response, query, top_k)

    def _text_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ–‡æœ¬æ£€ç´¢"""
        # åŠ¨æ€æ„å»ºæŸ¥è¯¢å­—æ®µ
        should_queries = []
        
        for field, boost in self.search_fields.items():
            should_queries.append({"match": {field: {"query": query, "boost": boost, "analyzer": self.search_analyzer}}})
        
        # ğŸ”§ ä½¿ç”¨æ–°çš„é«˜äº®å­—æ®µæ„å»ºæ–¹æ³•
        highlight_fields = self._build_highlight_fields()
        
        search_body = {
            "query": {"bool": {"should": should_queries}},
            "highlight": {
                "fields": highlight_fields,
                "require_field_match": self.default_highlight_settings["require_field_match"]
            },
            "size": top_k,
        }
        
        if self.debug:
            self.logger.debug(f"ğŸ“ æ–‡æœ¬æ£€ç´¢æŸ¥è¯¢ä½“: {search_body}")
        
        response = self.client.search(index=self.index_name, body=search_body)
        return self._format_results_with_highlights(response, query, "æ–‡æœ¬æ£€ç´¢")

    def _vector_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """å‘é‡æ£€ç´¢"""
        if not self.embedding_client:
            raise RuntimeError("å‘é‡æ£€ç´¢éœ€è¦embeddingå®¢æˆ·ç«¯")

        # è·å–å‘é‡å­—æ®µå
        vector_field = self.embedding_config.get("field_name", "embedding")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        if self.debug:
            self.logger.debug(f"ğŸ”¢ æ­£åœ¨ç”Ÿæˆå‘é‡æ£€ç´¢æŸ¥è¯¢å‘é‡: '{query}'")

        query_embedding = self.embedding_client.embed_text(query)

        if self.debug:
            self.logger.debug(
                f"âœ… å‘é‡æ£€ç´¢æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {len(query_embedding)}"
            )

        # å‘é‡æ£€ç´¢
        vector_search_body = {
            "knn": {
                "field": vector_field,
                "query_vector": query_embedding,
                "k": top_k,
                "num_candidates": top_k * 4,
            },
            "size": top_k,
        }

        if self.debug:
            self.logger.debug(f"ğŸ¯ å‘é‡æ£€ç´¢æŸ¥è¯¢ä½“: {vector_search_body}")

        response = self.client.search(index=self.index_name, body=vector_search_body)
        return self._format_results_with_highlights(response, query, "å‘é‡æ£€ç´¢")

    def _text_search_with_filter(self, query: str, top_k: int, filter_clause: Dict) -> List[Dict[str, Any]]:
        """å¸¦è¿‡æ»¤å™¨çš„æ–‡æœ¬æ£€ç´¢"""
        # åŠ¨æ€æ„å»ºæŸ¥è¯¢å­—æ®µ
        should_queries = []
        
        for field, boost in self.search_fields.items():
            should_queries.append({"match": {field: {"query": query, "boost": boost, "analyzer": self.search_analyzer}}})

        # ä½¿ç”¨åŠ¨æ€é«˜äº®å­—æ®µæ„å»ºæ–¹æ³•
        highlight_fields = self._build_highlight_fields()

        search_body = {
            "query": {
                "bool": {
                    "should": should_queries,
                    "filter": filter_clause
                }
            },
            "highlight": {
                "fields": highlight_fields,
                "require_field_match": self.highlight_config.get("require_field_match", False)
            },
            "size": top_k,
        }

        if self.debug:
            self.logger.debug(f"ğŸ“ å¸¦è¿‡æ»¤å™¨çš„æ–‡æœ¬æ£€ç´¢æŸ¥è¯¢ä½“: {search_body}")

        response = self.client.search(index=self.index_name, body=search_body)
        return self._format_results_with_highlights(response, query, "æ–‡æœ¬æ£€ç´¢")

    def _detect_chunk_level_config(self) -> Optional[Dict[str, Any]]:
        """åŠ¨æ€æ£€æµ‹chunk_levelé…ç½®"""
        try:
            # æ£€æµ‹å­—æ®µè·¯å¾„
            possible_paths = ['chunk_level', 'metadata.chunk_level']
            
            for path in possible_paths:
                test_query = {
                    "query": {"exists": {"field": path}},
                    "size": 1
                }
                
                result = self.client.search(index=self.index_name, body=test_query)
                if result['hits']['total']['value'] > 0:
                    # è·å–è¯¥å­—æ®µçš„æ‰€æœ‰å€¼
                    agg_query = {
                        "size": 0,
                        "aggs": {
                            "levels": {
                                "terms": {"field": path, "size": 10}
                            }
                        }
                    }
                    
                    agg_result = self.client.search(index=self.index_name, body=agg_query)
                    levels = [bucket['key'] for bucket in agg_result['aggregations']['levels']['buckets']]
                    
                    if levels:
                        # é€‰æ‹©æœ€å¤§çš„levelä½œä¸ºsmall chunk level
                        small_chunk_level = max(levels)
                        
                        return {
                            "field_path": path,
                            "small_chunk_level": small_chunk_level,
                            "available_levels": levels
                        }
            
            return None
            
        except Exception as e:
            if self.debug:
                self.logger.error(f"æ£€æµ‹chunk_levelé…ç½®å¤±è´¥: {e}")
            return None

    def _vector_search_with_filter(self, query: str, top_k: int, filter_clause: Dict) -> List[Dict[str, Any]]:
        """å¸¦è¿‡æ»¤å™¨çš„å‘é‡æ£€ç´¢"""
        if not self.embedding_client:
            raise RuntimeError("å‘é‡æ£€ç´¢éœ€è¦embeddingå®¢æˆ·ç«¯")

        # è·å–å‘é‡å­—æ®µå
        vector_field = self.embedding_config.get("field_name", "embedding")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        if self.debug:
            self.logger.debug(f"ğŸ”¢ æ­£åœ¨ç”Ÿæˆå‘é‡æ£€ç´¢æŸ¥è¯¢å‘é‡: '{query}'")

        query_embedding = self.embedding_client.embed_text(query)

        if self.debug:
            self.logger.debug(
                f"âœ… å‘é‡æ£€ç´¢æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {len(query_embedding)}"
            )

        # å‘é‡æ£€ç´¢
        vector_search_body = {
            "knn": {
                "field": vector_field,
                "query_vector": query_embedding,
                "k": top_k,
                "num_candidates": top_k * 4,
                "filter": filter_clause
            },
            "size": top_k,
        }

        if self.debug:
            self.logger.debug(f"ğŸ¯ å¸¦è¿‡æ»¤å™¨çš„å‘é‡æ£€ç´¢æŸ¥è¯¢ä½“: {vector_search_body}")

        response = self.client.search(index=self.index_name, body=vector_search_body)
        return self._format_results_with_highlights(response, query, "å‘é‡æ£€ç´¢")

    def _format_results_with_highlights(
        self, response, query: str = "", search_type: str = ""
    ) -> List[Dict[str, Any]]:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœå¹¶æ˜¾ç¤ºå‘½ä¸­è¯æ±‡"""
        results = []
        total_hits = response["hits"]["total"]

        if self.debug:
            if isinstance(total_hits, dict):
                total_count = total_hits.get("value", 0)
            else:
                total_count = total_hits
            self.logger.debug(
                f"ğŸ“Š {search_type} åŸå§‹æ£€ç´¢ç»“æœç»Ÿè®¡ - æ€»å‘½ä¸­æ•°: {total_count}, è¿”å›æ–‡æ¡£æ•°: {len(response['hits']['hits'])}"
            )

        for i, hit in enumerate(response["hits"]["hits"]):
            # æå–é«˜äº®ä¿¡æ¯
            highlights = hit.get("highlight", {})
            matched_terms = self._extract_matched_terms(highlights, query)
            
            result = {
                "id": hit["_id"],
                "score": hit["_score"],
                "content": hit["_source"].get("content", ""),
                "metadata": hit["_source"].get("metadata", {}),
                "recall_source": "text" if search_type == "æ–‡æœ¬æ£€ç´¢" else "vector",
                "highlights": matched_terms,  # æ·»åŠ é«˜äº®ä¿¡æ¯åˆ°ç»“æœä¸­
            }

            # è¿‡æ»¤ä½åˆ†ç»“æœ
            if result["score"] >= self.similarity_threshold:
                results.append(result)

                if self.debug:
                    # æ‰“å°è¯¦ç»†çš„å‘½ä¸­æ–‡æ¡£ä¿¡æ¯
                    content_preview = (
                        result["content"][:200] + "..."
                        if len(result["content"]) > 200
                        else result["content"]
                    )
                    metadata_info = (
                        ", ".join([f"{k}: {v}" for k, v in result["metadata"].items()])
                        if result["metadata"]
                        else "æ— å…ƒæ•°æ®"
                    )
                    recall_icon = "ğŸ“" if result["recall_source"] == "text" else "ğŸ¯"

                    self.logger.debug(
                        f"ğŸ“„ {recall_icon} {result['recall_source'].upper()}å¬å›æ–‡æ¡£ #{i+1}:"
                    )
                    self.logger.debug(f"   ğŸ“‹ æ–‡æ¡£ID: {result['id']}")
                    self.logger.debug(f"   â­ ç›¸ä¼¼åº¦åˆ†æ•°: {result['score']:.4f}")

                    # æ˜¾ç¤ºå‘½ä¸­çš„è¯æ±‡å’Œå­—æ®µ
                    if matched_terms:
                        self.logger.debug("   ğŸ¯ å‘½ä¸­è¯æ±‡è¯¦æƒ…:")
                        for field, terms in matched_terms.items():
                            if terms and isinstance(terms, dict):
                                # ä¼˜å…ˆæ˜¾ç¤ºç›¸å…³è¯æ±‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºæ‰€æœ‰è¯æ±‡
                                relevant_terms = terms.get('relevant_terms', [])
                                all_terms = terms.get('all_terms', [])
                                display_terms = relevant_terms if relevant_terms else all_terms
                                if display_terms:
                                    self.logger.debug(
                                        f"      ğŸ“ {field}: {', '.join(display_terms)}"
                                    )
                            elif isinstance(terms, list) and terms:
                                # å…¼å®¹æ—§æ ¼å¼ï¼ˆå¦‚æœæ˜¯åˆ—è¡¨ï¼‰
                                self.logger.debug(
                                    f"      ğŸ“ {field}: {', '.join(terms)}"
                                )

                        # æ˜¾ç¤ºé«˜äº®ç‰‡æ®µ
                        if highlights:
                            self.logger.debug("   ğŸ’¡ é«˜äº®ç‰‡æ®µ:")
                            for field, fragments in highlights.items():
                                if fragments:
                                    for j, fragment in enumerate(
                                        fragments[:2], 1
                                    ):  # æœ€å¤šæ˜¾ç¤º2ä¸ªç‰‡æ®µ
                                        clean_fragment = fragment.replace(
                                            "<mark>", "ã€"
                                        ).replace("</mark>", "ã€‘")
                                        self.logger.debug(
                                            f"      {j}. {field}: {clean_fragment}"
                                        )
                    else:
                        self.logger.debug("   â“ æœªæ‰¾åˆ°å…·ä½“å‘½ä¸­è¯æ±‡ï¼ˆå¯èƒ½æ˜¯æ¨¡ç³ŠåŒ¹é…ï¼‰")

                    self.logger.debug(f"   ğŸ“ å†…å®¹é¢„è§ˆ: {content_preview}")
                    self.logger.debug(f"   ğŸ·ï¸  å…ƒæ•°æ®: {metadata_info}")
                    self.logger.debug(f"   ğŸ“ å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
            else:
                if self.debug:
                    self.logger.debug(
                        f"âŒ æ–‡æ¡£ {hit['_id']} åˆ†æ•° {hit['_score']:.4f} ä½äºé˜ˆå€¼ {self.similarity_threshold}ï¼Œå·²è¿‡æ»¤"
                    )

        if self.debug:
            self.logger.debug(
                f"ğŸ¯ {search_type} æœ€ç»ˆè¿”å› {len(results)} ä¸ªæœ‰æ•ˆç»“æœ (æŸ¥è¯¢: '{query}')"
            )
            if results:
                avg_score = sum(r["score"] for r in results) / len(results)
                max_score = max(r["score"] for r in results)
                min_score = min(r["score"] for r in results)
                self.logger.debug(
                    f"ğŸ“ˆ åˆ†æ•°ç»Ÿè®¡ - æœ€é«˜: {max_score:.4f}, æœ€ä½: {min_score:.4f}, å¹³å‡: {avg_score:.4f}"
                )

        return results

    def _extract_matched_terms(self, highlights: dict, query: str) -> dict:
        """ä»é«˜äº®ä¿¡æ¯ä¸­æå–åŒ¹é…çš„è¯æ±‡"""
        matched_terms = {}
        
        # åˆ†ææŸ¥è¯¢è¯
        query_terms = set(query.lower().split())
    
        for field, fragments in highlights.items():
            field_terms = set()
            relevant_terms = set()  # ä¸æŸ¥è¯¢ç›¸å…³çš„è¯æ±‡
    
            for fragment in fragments:
                # æå–è¢«æ ‡è®°çš„è¯æ±‡
                import re
    
                marked_words = re.findall(r'<mark>(.*?)</mark>', fragment)
                for word in marked_words:
                    # æ¸…ç†å’Œæ ‡å‡†åŒ–è¯æ±‡
                    clean_word = re.sub(r'[^\w\u4e00-\u9fff]', '', word.lower())
                    if clean_word:
                        field_terms.add(clean_word)
                        # æ£€æŸ¥æ˜¯å¦ä¸æŸ¥è¯¢è¯ç›¸å…³ï¼ˆå®Œå…¨åŒ¹é…æˆ–åŒ…å«å…³ç³»ï¼‰
                        if any(clean_word in qterm or qterm in clean_word for qterm in query_terms):
                            relevant_terms.add(clean_word)
    
            if field_terms:
                matched_terms[field] = {
                    'all_terms': list(field_terms),
                    'relevant_terms': list(relevant_terms)
                }
    
        return matched_terms

    def _hybrid_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢ï¼ˆæ–‡æœ¬+å‘é‡ï¼‰- æ”¯æŒRRFå’ŒåŠ æƒèåˆ"""
        if not self.embedding_client:
            raise RuntimeError("æ··åˆæ£€ç´¢éœ€è¦embeddingå®¢æˆ·ç«¯")
    
        # è·å–å‘é‡å­—æ®µå
        vector_field = self.embedding_config.get("field_name", "embedding")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        if self.debug:
            self.logger.debug(f"ğŸ”¢ æ­£åœ¨ç”Ÿæˆæ··åˆæ£€ç´¢æŸ¥è¯¢å‘é‡: '{query}'")

        query_embedding = self.embedding_client.embed_text(query)

        if self.debug:
            self.logger.debug(
                f"âœ… æ··åˆæ£€ç´¢æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {len(query_embedding)}"
            )
            self.logger.debug(f"ğŸ” å¼€å§‹æ‰§è¡Œæ··åˆæ£€ç´¢ - èåˆæ–¹æ³•: {self.fusion_method}")

        # 1. æ–‡æœ¬æ£€ç´¢ï¼ˆå¸¦é«˜äº®ï¼‰
        should_queries = []
        highlight_fields = {}

        for field, boost in self.search_fields.items():
            # å¯¹äºæ··åˆæ£€ç´¢ï¼Œé€‚å½“é™ä½æ–‡æœ¬æ£€ç´¢çš„æƒé‡
            adjusted_boost = boost * 0.8  # é™ä½20%æƒé‡ç»™å‘é‡æ£€ç´¢ç•™ç©ºé—´
            should_queries.append(
                {"match": {field: {"query": query, "boost": adjusted_boost, "analyzer": self.search_analyzer}}}
            )

            # ä¸ºæ‰€æœ‰æŸ¥è¯¢å­—æ®µæ·»åŠ é«˜äº®ï¼ˆä¹‹å‰åªä¸ºéƒ¨åˆ†å­—æ®µæ·»åŠ ï¼‰
            if field in ["content", "content_jieba"]:
                highlight_fields[field] = {
                    "fragment_size": 150,
                    "number_of_fragments": 2,
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"],
                }
            elif "title" in field or "file_path" in field:
                highlight_fields[field] = {
                    "fragment_size": 100,
                    "number_of_fragments": 1,
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"],
                }
            else:
                # ä¸ºå…¶ä»–æ‰€æœ‰å­—æ®µä¹Ÿæ·»åŠ é«˜äº®è®¾ç½®
                highlight_fields[field] = {
                    "fragment_size": 100,
                    "number_of_fragments": 1,
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"],
                }

        text_search_body = {
            "query": {"bool": {"should": should_queries}},
            "highlight": {"fields": highlight_fields},
            "size": top_k * 2,
        }

        if self.debug:
            self.logger.debug(f"ğŸ“ æ··åˆæ£€ç´¢-æ–‡æœ¬éƒ¨åˆ†æŸ¥è¯¢ä½“: {text_search_body}")

        text_response = self.client.search(index=self.index_name, body=text_search_body)

        # 2. å‘é‡æ£€ç´¢
        vector_search_body = {
            "knn": {
                "field": vector_field,
                "query_vector": query_embedding,
                "k": top_k * 2,
                "num_candidates": top_k * 4,
            },
            "size": top_k * 2,
        }

        if self.debug:
            self.logger.debug(f"ğŸ¯ æ··åˆæ£€ç´¢-å‘é‡éƒ¨åˆ†æŸ¥è¯¢ä½“: {vector_search_body}")

        vector_response = self.client.search(
            index=self.index_name, body=vector_search_body
        )

        # æ ¹æ®èåˆæ–¹æ³•é€‰æ‹©åˆå¹¶ç­–ç•¥
        if self.fusion_method == "rrf":
            return self._merge_hybrid_results_with_rrf(
                text_response, vector_response, query, top_k
            )
        else:
            return self._merge_hybrid_results_with_highlights(
                text_response, vector_response, query, top_k
            )

    def _merge_hybrid_results_with_rrf(
        self, text_response, vector_response, query: str, top_k: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨RRFç®—æ³•åˆå¹¶æ··åˆæ£€ç´¢ç»“æœ"""
        if self.debug:
            text_hits = len(text_response["hits"]["hits"])
            vector_hits = len(vector_response["hits"]["hits"])
            self.logger.debug(
                f"ğŸ”„ å¼€å§‹RRFåˆå¹¶æ··åˆæ£€ç´¢ç»“æœ - æ–‡æœ¬æ£€ç´¢: {text_hits}ä¸ª, å‘é‡æ£€ç´¢: {vector_hits}ä¸ª, RRF-K: {self.rrf_k}"
            )

        # æ„å»ºæ–‡æ¡£æ˜ å°„å’Œæ’å
        text_docs = {}
        vector_docs = {}
        text_ranks = {}
        vector_ranks = {}
        highlights_map = {}

        # å¤„ç†æ–‡æœ¬æ£€ç´¢ç»“æœ
        for rank, hit in enumerate(text_response["hits"]["hits"], 1):
            doc_id = hit["_id"]
            text_docs[doc_id] = {
                "id": doc_id,
                "content": hit["_source"].get("content", ""),
                "metadata": hit["_source"].get("metadata", {}),
                "text_score": hit["_score"],
                "text_rank": rank,
            }
            text_ranks[doc_id] = rank
            highlights_map[doc_id] = hit.get("highlight", {})

            if self.debug:
                matched_terms = self._extract_matched_terms(highlights_map[doc_id], query)
                terms_info = []
                for field, terms in matched_terms.items():
                    if terms and isinstance(terms, dict):
                        relevant_terms = terms.get('relevant_terms', [])
                        all_terms = terms.get('all_terms', [])
                        # ä¼˜å…ˆæ˜¾ç¤ºç›¸å…³è¯æ±‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºæ‰€æœ‰é«˜äº®è¯æ±‡
                        display_terms = relevant_terms if relevant_terms else all_terms
                        if display_terms:
                            terms_info.append(f"{field}: {', '.join(display_terms)}")
                    elif terms and isinstance(terms, list):
                        terms_info.append(f"{field}: {', '.join(terms)}")
                
                final_terms_info = ', '.join(terms_info) if terms_info else 'æ— '
                # æ·»åŠ é«˜äº®ä¿¡æ¯è°ƒè¯•
                highlight_info = highlights_map[doc_id]
                self.logger.debug(f"ğŸ” æ–‡æ¡£ {doc_id} çš„é«˜äº®ä¿¡æ¯: {highlight_info}")
                
                matched_terms = self._extract_matched_terms(highlights_map[doc_id], query)
                self.logger.debug(f"ğŸ¯ æ–‡æ¡£ {doc_id} æå–çš„åŒ¹é…è¯: {matched_terms}")
                
                terms_info = []
                for field, terms in matched_terms.items():
                    if terms and isinstance(terms, dict):
                        relevant_terms = terms.get('relevant_terms', [])
                        all_terms = terms.get('all_terms', [])
                        # ä¼˜å…ˆæ˜¾ç¤ºç›¸å…³è¯æ±‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºæ‰€æœ‰é«˜äº®è¯æ±‡
                        display_terms = relevant_terms if relevant_terms else all_terms
                        self.logger.debug(f"ğŸ“‹ å­—æ®µ {field}: relevant_terms={relevant_terms}, all_terms={all_terms}, display_terms={display_terms}")
                        if display_terms:
                            terms_info.append(f"{field}: {', '.join(display_terms)}")
                    elif terms and isinstance(terms, list):
                        terms_info.append(f"{field}: {', '.join(terms)}")
                
                final_terms_info = ', '.join(terms_info) if terms_info else 'æ— '
                self.logger.debug(
                    f"ğŸ“ æ–‡æœ¬å¬å›æ–‡æ¡£: {doc_id}, æ’å: {rank}, åˆ†æ•°: {hit['_score']:.4f}, å‘½ä¸­è¯: {final_terms_info}"
                )

        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        for rank, hit in enumerate(vector_response["hits"]["hits"], 1):
            doc_id = hit["_id"]
            vector_docs[doc_id] = {
                "id": doc_id,
                "content": hit["_source"].get("content", ""),
                "metadata": hit["_source"].get("metadata", {}),
                "vector_score": hit["_score"],
                "vector_rank": rank,
            }
            vector_ranks[doc_id] = rank

            if self.debug:
                self.logger.debug(
                    f"ğŸ¯ å‘é‡å¬å›æ–‡æ¡£: {doc_id}, æ’å: {rank}, åˆ†æ•°: {hit['_score']:.4f}"
                )

        # è®¡ç®—RRFåˆ†æ•°
        all_doc_ids = set(text_ranks.keys()) | set(vector_ranks.keys())
        rrf_results = []

        for doc_id in all_doc_ids:
            text_rank = text_ranks.get(doc_id, float('inf'))
            vector_rank = vector_ranks.get(doc_id, float('inf'))
            
            # è®¡ç®—RRFåˆ†æ•°
            rrf_score = 0
            if text_rank != float('inf'):
                rrf_score += 1 / (self.rrf_k + text_rank)
            if vector_rank != float('inf'):
                rrf_score += 1 / (self.rrf_k + vector_rank)
            
            # ç¡®å®šå¬å›æ¥æº
            if text_rank != float('inf') and vector_rank != float('inf'):
                recall_source = "hybrid"
                doc_info = {**text_docs[doc_id], **vector_docs[doc_id]}
            elif text_rank != float('inf'):
                recall_source = "text"
                doc_info = {**text_docs[doc_id], "vector_score": 0.0, "vector_rank": None}
            else:
                recall_source = "vector"
                doc_info = {**vector_docs[doc_id], "text_score": 0.0, "text_rank": None}
                highlights_map[doc_id] = {}  # å‘é‡æ£€ç´¢æ²¡æœ‰é«˜äº®
            
            result = {
                "id": doc_id,
                "score": rrf_score,  # ä½¿ç”¨RRFåˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
                "content": doc_info["content"],
                "metadata": doc_info["metadata"],
                "recall_source": recall_source,
                "text_score": doc_info.get("text_score", 0.0),
                "vector_score": doc_info.get("vector_score", 0.0),
                "text_rank": doc_info.get("text_rank"),
                "vector_rank": doc_info.get("vector_rank"),
                "rrf_score": rrf_score,
                "highlights": self._extract_matched_terms(highlights_map.get(doc_id, {}), query),
            }
            
            rrf_results.append(result)

        # æŒ‰RRFåˆ†æ•°æ’åº
        rrf_results.sort(key=lambda x: x["rrf_score"], reverse=True)
        
        # å–top_kå¹¶åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        final_results = []
        for result in rrf_results[:top_k]:
            # å¯¹äºRRFï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ç­–ç•¥
            # å› ä¸ºRRFåˆ†æ•°çš„å°ºåº¦ä¸åŸå§‹åˆ†æ•°ä¸åŒ
            if result["rrf_score"] > 0:  # RRFåˆ†æ•°å¤§äº0å³è¡¨ç¤ºæœ‰æ„ä¹‰
                final_results.append(result)

        if self.debug:
            self.logger.debug("ğŸ“Š RRFæ··åˆæ£€ç´¢ç»“æœç»Ÿè®¡:")
            text_only = sum(1 for r in final_results if r["recall_source"] == "text")
            vector_only = sum(1 for r in final_results if r["recall_source"] == "vector")
            hybrid_both = sum(1 for r in final_results if r["recall_source"] == "hybrid")

            self.logger.debug(f"   ğŸ“ ä»…æ–‡æœ¬å¬å›: {text_only}ä¸ª")
            self.logger.debug(f"   ğŸ¯ ä»…å‘é‡å¬å›: {vector_only}ä¸ª")
            self.logger.debug(f"   ğŸ”„ æ··åˆå¬å›(æ–‡æœ¬+å‘é‡): {hybrid_both}ä¸ª")
            self.logger.debug(f"   ğŸ¯ æœ€ç»ˆè¿”å›: {len(final_results)}ä¸ªæ–‡æ¡£")

            # è¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„RRFä¿¡æ¯
            for i, result in enumerate(final_results, 1):
                content_preview = (
                    result["content"][:100] + "..."
                    if len(result["content"]) > 100
                    else result["content"]
                )

                self.logger.debug(f"ğŸ“„ RRFæ’å#{i} - æ–‡æ¡£ID: {result['id']}")
                
                # æ˜¾ç¤ºRRFè®¡ç®—è¯¦æƒ…
                rrf_detail = f"RRFåˆ†æ•°: {result['rrf_score']:.6f}"
                if result["text_rank"] and result["vector_rank"]:
                    rrf_detail += f" (æ–‡æœ¬æ’å: {result['text_rank']}, å‘é‡æ’å: {result['vector_rank']})"
                elif result["text_rank"]:
                    rrf_detail += f" (ä»…æ–‡æœ¬æ’å: {result['text_rank']})"
                else:
                    rrf_detail += f" (ä»…å‘é‡æ’å: {result['vector_rank']})"
                
                self.logger.debug(f"   ğŸ”¢ {rrf_detail}")
                
                # æ˜¾ç¤ºåŸå§‹åˆ†æ•°
                if result["text_score"] > 0 and result["vector_score"] > 0:
                    self.logger.debug(f"   ğŸ“Š åŸå§‹åˆ†æ•° - æ–‡æœ¬: {result['text_score']:.4f}, å‘é‡: {result['vector_score']:.4f}")
                elif result["text_score"] > 0:
                    self.logger.debug(f"   ğŸ“Š åŸå§‹åˆ†æ•° - æ–‡æœ¬: {result['text_score']:.4f}")
                else:
                    self.logger.debug(f"   ğŸ“Š åŸå§‹åˆ†æ•° - å‘é‡: {result['vector_score']:.4f}")
                
                # æ˜¾ç¤ºå‘½ä¸­è¯æ±‡ï¼ˆä»…æ–‡æœ¬å¬å›æœ‰ï¼‰
                if result["recall_source"] in ["text", "hybrid"] and result["highlights"]:
                    terms_info = []
                    for field, terms in result["highlights"].items():
                        if terms and isinstance(terms, dict):
                            relevant_terms = terms.get('relevant_terms', [])
                            if relevant_terms:  # åªæœ‰å½“æœ‰ç›¸å…³è¯æ±‡æ—¶æ‰æ·»åŠ 
                                terms_info.append(f"{field}: {', '.join(relevant_terms)}")
                        elif terms and isinstance(terms, list):
                            terms_info.append(f"{field}: {', '.join(terms)}")
                    
                    if terms_info:
                        self.logger.debug(f"   ğŸ¯ å‘½ä¸­è¯æ±‡: {', '.join(terms_info)}")
                
                self.logger.debug(f"   ğŸ·ï¸  å¬å›æ–¹å¼: {result['recall_source']}")
                self.logger.debug(f"   ğŸ“ å†…å®¹: {content_preview}")

        return final_results

    def _merge_hybrid_results_with_highlights(
        self, text_response, vector_response, query: str, top_k: int
    ) -> List[Dict[str, Any]]:
        """åˆå¹¶æ··åˆæ£€ç´¢ç»“æœå¹¶æ ‡è®°æ¥æºï¼ˆåŒ…å«é«˜äº®ä¿¡æ¯ï¼‰- åŠ æƒèåˆæ–¹æ³•"""
        results_map = {}
        highlights_map = {}  # å­˜å‚¨é«˜äº®ä¿¡æ¯

        if self.debug:
            text_hits = len(text_response["hits"]["hits"])
            vector_hits = len(vector_response["hits"]["hits"])
            self.logger.debug(
                f"ğŸ”„ å¼€å§‹åŠ æƒåˆå¹¶æ··åˆæ£€ç´¢ç»“æœ - æ–‡æœ¬æ£€ç´¢: {text_hits}ä¸ª, å‘é‡æ£€ç´¢: {vector_hits}ä¸ª"
            )

        # å¤„ç†æ–‡æœ¬æ£€ç´¢ç»“æœ
        for hit in text_response["hits"]["hits"]:
            doc_id = hit["_id"]
            score = hit["_score"]
            highlights = hit.get("highlight", {})

            result = {
                "id": doc_id,
                "score": score,
                "content": hit["_source"].get("content", ""),
                "metadata": hit["_source"].get("metadata", {}),
                "recall_source": "text",
                "text_score": score,
                "vector_score": 0.0,
                "hybrid_score": score * self.text_weight,
            }

            results_map[doc_id] = result
            highlights_map[doc_id] = highlights

            if self.debug:
                matched_terms = self._extract_matched_terms(highlights, query)
                terms_info = ", ".join([
                    f"{field}: {', '.join(terms['relevant_terms']) if isinstance(terms, dict) else str(terms)}"
                    for field, terms in matched_terms.items()
                    if terms
                ])
                self.logger.debug(
                    f"ğŸ“ æ–‡æœ¬å¬å›æ–‡æ¡£: {doc_id}, åˆ†æ•°: {score:.4f}, å‘½ä¸­è¯: {terms_info or 'æ— '}"
                )

        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        for hit in vector_response["hits"]["hits"]:
            doc_id = hit["_id"]
            score = hit["_score"]

            if doc_id in results_map:
                # æ–‡æ¡£åŒæ—¶è¢«æ–‡æœ¬å’Œå‘é‡å¬å›
                results_map[doc_id]["recall_source"] = "hybrid"
                results_map[doc_id]["vector_score"] = score
                results_map[doc_id]["hybrid_score"] = (
                    results_map[doc_id]["text_score"] * self.text_weight + score * self.vector_weight
                )

                if self.debug:
                    highlights = highlights_map.get(doc_id, {})
                    matched_terms = self._extract_matched_terms(highlights, query)
                    terms_info = ", ".join([
                        f"{field}: {', '.join(terms['relevant_terms']) if isinstance(terms, dict) else str(terms)}"
                        for field, terms in matched_terms.items()
                        if terms
                    ])
                    self.logger.debug(
                        f"ğŸ¯ æ··åˆå¬å›æ–‡æ¡£: {doc_id}, æ–‡æœ¬åˆ†æ•°: {results_map[doc_id]['text_score']:.4f}, å‘é‡åˆ†æ•°: {score:.4f}, æ··åˆåˆ†æ•°: {results_map[doc_id]['hybrid_score']:.4f}, å‘½ä¸­è¯: {terms_info or 'æ— '}"
                    )
            else:
                # ä»…è¢«å‘é‡å¬å›
                result = {
                    "id": doc_id,
                    "score": score,
                    "content": hit["_source"].get("content", ""),
                    "metadata": hit["_source"].get("metadata", {}),
                    "recall_source": "vector",
                    "text_score": 0.0,
                    "vector_score": score,
                    "hybrid_score": score * self.vector_weight,
                }

                results_map[doc_id] = result
                highlights_map[doc_id] = {}

                if self.debug:
                    self.logger.debug(f"ğŸ¯ å‘é‡å¬å›æ–‡æ¡£: {doc_id}, åˆ†æ•°: {score:.4f}")

        # å…ˆè¿›è¡Œç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼Œå†æŒ‰æ··åˆåˆ†æ•°æ’åºå¹¶å–top_k
        filtered_results = []
        for result in results_map.values():
            result["score"] = result["hybrid_score"]
            # å¯¹äºæ··åˆæ£€ç´¢ï¼Œä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ç­–ç•¥
            threshold = self.similarity_threshold * 0.8  # é™ä½20%é˜ˆå€¼
            if result["score"] >= threshold:
                filtered_results.append(result)

        # æŒ‰æ··åˆåˆ†æ•°æ’åºå¹¶å–top_k
        filtered_results.sort(key=lambda x: x["hybrid_score"], reverse=True)
        final_results = filtered_results[:top_k]

        if self.debug:
            self.logger.debug("ğŸ“Š åŠ æƒæ··åˆæ£€ç´¢ç»“æœç»Ÿè®¡:")
            text_only = sum(1 for r in final_results if r["recall_source"] == "text")
            vector_only = sum(
                1 for r in final_results if r["recall_source"] == "vector"
            )
            hybrid_both = sum(
                1 for r in final_results if r["recall_source"] == "hybrid"
            )

            self.logger.debug(f"   ğŸ“ ä»…æ–‡æœ¬å¬å›: {text_only}ä¸ª")
            self.logger.debug(f"   ğŸ¯ ä»…å‘é‡å¬å›: {vector_only}ä¸ª")
            self.logger.debug(f"   ğŸ”„ æ··åˆå¬å›(æ–‡æœ¬+å‘é‡): {hybrid_both}ä¸ª")
            self.logger.debug(f"   ğŸ¯ æœ€ç»ˆè¿”å›: {len(final_results)}ä¸ªæ–‡æ¡£")

            # è¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„å¬å›ä¿¡æ¯
            for i, result in enumerate(final_results, 1):
                content_preview = (
                    result["content"][:100] + "..."
                    if len(result["content"]) > 100
                    else result["content"]
                )

                self.logger.debug(f"ğŸ“„ æ’å#{i} - æ–‡æ¡£ID: {result['id']}")

                # æ˜¾ç¤ºæ–‡æœ¬å¬å›çš„å‘½ä¸­è¯æ±‡
                if result["recall_source"] in ["text", "hybrid"]:
                    highlights = highlights_map.get(result["id"], {})
                    matched_terms = self._extract_matched_terms(highlights, query)
                    if matched_terms:
                        terms_info = ", ".join([
                            f"{field}: {', '.join(terms['relevant_terms']) if isinstance(terms, dict) else str(terms)}"
                            for field, terms in matched_terms.items()
                            if terms
                        ])
                        if terms_info:
                            self.logger.debug(f"   ğŸ¯ å‘½ä¸­è¯æ±‡: {terms_info}")

                source_info = f"å¬å›æ–¹å¼: {result['recall_source']}"
                if result["recall_source"] == "hybrid":
                    source_info += f" (æ–‡æœ¬: {result['text_score']:.4f}, å‘é‡: {result['vector_score']:.4f})"
                elif result["recall_source"] == "text":
                    source_info += f" (æ–‡æœ¬åˆ†æ•°: {result['text_score']:.4f})"
                else:
                    source_info += f" (å‘é‡åˆ†æ•°: {result['vector_score']:.4f})"

                self.logger.debug(f"   ğŸ·ï¸  {source_info}")
                self.logger.debug(f"   â­ æœ€ç»ˆåˆ†æ•°: {result['score']:.4f}")
                self.logger.debug(f"   ğŸ“ å†…å®¹: {content_preview}")

        return final_results
